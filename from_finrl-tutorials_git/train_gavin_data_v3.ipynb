{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317e332f-3b11-4572-8cbe-6a603ccc1907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993-01-04</td>\n",
       "      <td>ADMG</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993-01-04</td>\n",
       "      <td>ASII</td>\n",
       "      <td>108.20</td>\n",
       "      <td>110.00</td>\n",
       "      <td>108.20</td>\n",
       "      <td>109.40</td>\n",
       "      <td>5523860.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>108.200000</td>\n",
       "      <td>108.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993-01-04</td>\n",
       "      <td>BDMN</td>\n",
       "      <td>30350.00</td>\n",
       "      <td>30350.00</td>\n",
       "      <td>30350.00</td>\n",
       "      <td>30350.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>30350.000000</td>\n",
       "      <td>30350.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993-01-04</td>\n",
       "      <td>BNGA</td>\n",
       "      <td>6070.00</td>\n",
       "      <td>6070.00</td>\n",
       "      <td>6070.00</td>\n",
       "      <td>6070.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>6070.000000</td>\n",
       "      <td>6070.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993-01-04</td>\n",
       "      <td>BNII</td>\n",
       "      <td>2537.27</td>\n",
       "      <td>2537.27</td>\n",
       "      <td>2537.27</td>\n",
       "      <td>2537.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2537.270000</td>\n",
       "      <td>2537.270000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228295</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>SMGR</td>\n",
       "      <td>5900.00</td>\n",
       "      <td>5950.00</td>\n",
       "      <td>5900.00</td>\n",
       "      <td>5950.00</td>\n",
       "      <td>6695300.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>-77.012660</td>\n",
       "      <td>6470.763074</td>\n",
       "      <td>5799.236926</td>\n",
       "      <td>41.660961</td>\n",
       "      <td>-145.740281</td>\n",
       "      <td>26.024825</td>\n",
       "      <td>6160.000000</td>\n",
       "      <td>6194.166667</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228296</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>TKIM</td>\n",
       "      <td>6850.00</td>\n",
       "      <td>6950.00</td>\n",
       "      <td>6800.00</td>\n",
       "      <td>6850.00</td>\n",
       "      <td>1895200.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>76.026807</td>\n",
       "      <td>7065.827666</td>\n",
       "      <td>5951.672334</td>\n",
       "      <td>51.211188</td>\n",
       "      <td>137.327368</td>\n",
       "      <td>27.307389</td>\n",
       "      <td>6465.000000</td>\n",
       "      <td>6712.500000</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228297</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>TSPC</td>\n",
       "      <td>1940.00</td>\n",
       "      <td>1960.00</td>\n",
       "      <td>1940.00</td>\n",
       "      <td>1955.00</td>\n",
       "      <td>839100.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>-17.276449</td>\n",
       "      <td>2056.822728</td>\n",
       "      <td>1925.177272</td>\n",
       "      <td>48.258022</td>\n",
       "      <td>-117.575758</td>\n",
       "      <td>10.393875</td>\n",
       "      <td>2014.666667</td>\n",
       "      <td>1964.333333</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228298</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>UNSP</td>\n",
       "      <td>110.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>105.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>83600.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>0.606521</td>\n",
       "      <td>115.289621</td>\n",
       "      <td>104.410379</td>\n",
       "      <td>49.705775</td>\n",
       "      <td>11.675265</td>\n",
       "      <td>32.719668</td>\n",
       "      <td>108.366667</td>\n",
       "      <td>110.300000</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228299</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>UNTR</td>\n",
       "      <td>24175.00</td>\n",
       "      <td>24325.00</td>\n",
       "      <td>24025.00</td>\n",
       "      <td>24025.00</td>\n",
       "      <td>2820700.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>282.995972</td>\n",
       "      <td>24456.262361</td>\n",
       "      <td>22453.737639</td>\n",
       "      <td>55.237626</td>\n",
       "      <td>148.788927</td>\n",
       "      <td>30.400357</td>\n",
       "      <td>23266.666667</td>\n",
       "      <td>23039.166667</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228300 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date   tic     close      high       low      open     volume  \\\n",
       "0       1993-01-04  ADMG      1.00      1.00      1.00      1.00        1.0   \n",
       "1       1993-01-04  ASII    108.20    110.00    108.20    109.40  5523860.0   \n",
       "2       1993-01-04  BDMN  30350.00  30350.00  30350.00  30350.00        1.0   \n",
       "3       1993-01-04  BNGA   6070.00   6070.00   6070.00   6070.00        1.0   \n",
       "4       1993-01-04  BNII   2537.27   2537.27   2537.27   2537.27        1.0   \n",
       "...            ...   ...       ...       ...       ...       ...        ...   \n",
       "228295  2024-03-08  SMGR   5900.00   5950.00   5900.00   5950.00  6695300.0   \n",
       "228296  2024-03-08  TKIM   6850.00   6950.00   6800.00   6850.00  1895200.0   \n",
       "228297  2024-03-08  TSPC   1940.00   1960.00   1940.00   1955.00   839100.0   \n",
       "228298  2024-03-08  UNSP    110.00    112.00    105.00    110.00    83600.0   \n",
       "228299  2024-03-08  UNTR  24175.00  24325.00  24025.00  24025.00  2820700.0   \n",
       "\n",
       "         day        macd       boll_ub       boll_lb      rsi_30      cci_30  \\\n",
       "0          1    0.000000      1.000000      1.000000  100.000000  -66.666667   \n",
       "1          1    0.000000      1.000000      1.000000  100.000000  -66.666667   \n",
       "2          1    0.000000      1.000000      1.000000  100.000000  -66.666667   \n",
       "3          1    0.000000      1.000000      1.000000  100.000000  -66.666667   \n",
       "4          1    0.000000      1.000000      1.000000  100.000000  -66.666667   \n",
       "...      ...         ...           ...           ...         ...         ...   \n",
       "228295  7610  -77.012660   6470.763074   5799.236926   41.660961 -145.740281   \n",
       "228296  7610   76.026807   7065.827666   5951.672334   51.211188  137.327368   \n",
       "228297  7610  -17.276449   2056.822728   1925.177272   48.258022 -117.575758   \n",
       "228298  7610    0.606521    115.289621    104.410379   49.705775   11.675265   \n",
       "228299  7610  282.995972  24456.262361  22453.737639   55.237626  148.788927   \n",
       "\n",
       "             dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "0       100.000000      1.000000      1.000000    0.000000  \n",
       "1       100.000000    108.200000    108.200000    0.000000  \n",
       "2       100.000000  30350.000000  30350.000000    0.000000  \n",
       "3       100.000000   6070.000000   6070.000000    0.000000  \n",
       "4       100.000000   2537.270000   2537.270000    0.000000  \n",
       "...            ...           ...           ...         ...  \n",
       "228295   26.024825   6160.000000   6194.166667    8.296629  \n",
       "228296   27.307389   6465.000000   6712.500000    8.296629  \n",
       "228297   10.393875   2014.666667   1964.333333    8.296629  \n",
       "228298   32.719668    108.366667    110.300000    8.296629  \n",
       "228299   30.400357  23266.666667  23039.166667    8.296629  \n",
       "\n",
       "[228300 rows x 17 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fprocessed = \"/home/devmiftahul/trading_model/from_finrl-tutorials_git/processed_by_gavin/preprocessed_data_with_features.csv\"\n",
    "\n",
    "processed = pd.read_csv(fprocessed)\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384214ca-aea3-4314-99d0-ec55b704d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
    "from finrl.config import INDICATORS\n",
    "INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f48feeb0-8fa2-42a2-82bc-dd2721a95981",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICATORS = ['macd',\n",
    "               'rsi_30',\n",
    "               'cci_30',\n",
    "               'dx_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "579662fb-d11d-46ef-86de-e92df86eb7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7610"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed[\"day\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45de93cd-57a7-4626-8f7c-8d5ba0052c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226320</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>ADMG</td>\n",
       "      <td>144.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>206500.0</td>\n",
       "      <td>7545</td>\n",
       "      <td>-0.718494</td>\n",
       "      <td>150.296447</td>\n",
       "      <td>141.003553</td>\n",
       "      <td>46.974955</td>\n",
       "      <td>-92.964824</td>\n",
       "      <td>3.446503</td>\n",
       "      <td>146.266667</td>\n",
       "      <td>148.833333</td>\n",
       "      <td>55.067986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226321</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>5575.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>5575.0</td>\n",
       "      <td>161684600.0</td>\n",
       "      <td>7545</td>\n",
       "      <td>-93.982767</td>\n",
       "      <td>5964.262632</td>\n",
       "      <td>5463.237368</td>\n",
       "      <td>35.582688</td>\n",
       "      <td>-273.895582</td>\n",
       "      <td>46.098084</td>\n",
       "      <td>5725.833333</td>\n",
       "      <td>5940.000000</td>\n",
       "      <td>55.067986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226322</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>BDMN</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>2860.0</td>\n",
       "      <td>2830.0</td>\n",
       "      <td>2840.0</td>\n",
       "      <td>765400.0</td>\n",
       "      <td>7545</td>\n",
       "      <td>25.564362</td>\n",
       "      <td>2855.431444</td>\n",
       "      <td>2644.568556</td>\n",
       "      <td>55.241678</td>\n",
       "      <td>178.046812</td>\n",
       "      <td>47.754273</td>\n",
       "      <td>2723.666667</td>\n",
       "      <td>2777.166667</td>\n",
       "      <td>55.067986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226323</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>BNGA</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>1695.0</td>\n",
       "      <td>1695.0</td>\n",
       "      <td>4128300.0</td>\n",
       "      <td>7545</td>\n",
       "      <td>1.320446</td>\n",
       "      <td>1738.853248</td>\n",
       "      <td>1687.646752</td>\n",
       "      <td>51.672052</td>\n",
       "      <td>-18.920813</td>\n",
       "      <td>34.303349</td>\n",
       "      <td>1706.333333</td>\n",
       "      <td>1696.083333</td>\n",
       "      <td>55.067986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226324</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>BNII</td>\n",
       "      <td>256.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>1055100.0</td>\n",
       "      <td>7545</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>254.495589</td>\n",
       "      <td>239.704411</td>\n",
       "      <td>50.951895</td>\n",
       "      <td>160.990712</td>\n",
       "      <td>25.369015</td>\n",
       "      <td>247.466667</td>\n",
       "      <td>255.600000</td>\n",
       "      <td>55.067986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228295</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>SMGR</td>\n",
       "      <td>5900.0</td>\n",
       "      <td>5950.0</td>\n",
       "      <td>5900.0</td>\n",
       "      <td>5950.0</td>\n",
       "      <td>6695300.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>-77.012660</td>\n",
       "      <td>6470.763074</td>\n",
       "      <td>5799.236926</td>\n",
       "      <td>41.660961</td>\n",
       "      <td>-145.740281</td>\n",
       "      <td>26.024825</td>\n",
       "      <td>6160.000000</td>\n",
       "      <td>6194.166667</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228296</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>TKIM</td>\n",
       "      <td>6850.0</td>\n",
       "      <td>6950.0</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>6850.0</td>\n",
       "      <td>1895200.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>76.026807</td>\n",
       "      <td>7065.827666</td>\n",
       "      <td>5951.672334</td>\n",
       "      <td>51.211188</td>\n",
       "      <td>137.327368</td>\n",
       "      <td>27.307389</td>\n",
       "      <td>6465.000000</td>\n",
       "      <td>6712.500000</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228297</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>TSPC</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>839100.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>-17.276449</td>\n",
       "      <td>2056.822728</td>\n",
       "      <td>1925.177272</td>\n",
       "      <td>48.258022</td>\n",
       "      <td>-117.575758</td>\n",
       "      <td>10.393875</td>\n",
       "      <td>2014.666667</td>\n",
       "      <td>1964.333333</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228298</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>UNSP</td>\n",
       "      <td>110.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>83600.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>0.606521</td>\n",
       "      <td>115.289621</td>\n",
       "      <td>104.410379</td>\n",
       "      <td>49.705775</td>\n",
       "      <td>11.675265</td>\n",
       "      <td>32.719668</td>\n",
       "      <td>108.366667</td>\n",
       "      <td>110.300000</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228299</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>UNTR</td>\n",
       "      <td>24175.0</td>\n",
       "      <td>24325.0</td>\n",
       "      <td>24025.0</td>\n",
       "      <td>24025.0</td>\n",
       "      <td>2820700.0</td>\n",
       "      <td>7610</td>\n",
       "      <td>282.995972</td>\n",
       "      <td>24456.262361</td>\n",
       "      <td>22453.737639</td>\n",
       "      <td>55.237626</td>\n",
       "      <td>148.788927</td>\n",
       "      <td>30.400357</td>\n",
       "      <td>23266.666667</td>\n",
       "      <td>23039.166667</td>\n",
       "      <td>8.296629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1980 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date   tic    close     high      low     open       volume  \\\n",
       "226320 2023-11-30  ADMG    144.0    148.0    140.0    148.0     206500.0   \n",
       "226321 2023-11-30  ASII   5400.0   5575.0   5400.0   5575.0  161684600.0   \n",
       "226322 2023-11-30  BDMN   2850.0   2860.0   2830.0   2840.0     765400.0   \n",
       "226323 2023-11-30  BNGA   1705.0   1710.0   1695.0   1695.0    4128300.0   \n",
       "226324 2023-11-30  BNII    256.0    256.0    250.0    254.0    1055100.0   \n",
       "...           ...   ...      ...      ...      ...      ...          ...   \n",
       "228295 2024-03-08  SMGR   5900.0   5950.0   5900.0   5950.0    6695300.0   \n",
       "228296 2024-03-08  TKIM   6850.0   6950.0   6800.0   6850.0    1895200.0   \n",
       "228297 2024-03-08  TSPC   1940.0   1960.0   1940.0   1955.0     839100.0   \n",
       "228298 2024-03-08  UNSP    110.0    112.0    105.0    110.0      83600.0   \n",
       "228299 2024-03-08  UNTR  24175.0  24325.0  24025.0  24025.0    2820700.0   \n",
       "\n",
       "         day        macd       boll_ub       boll_lb     rsi_30      cci_30  \\\n",
       "226320  7545   -0.718494    150.296447    141.003553  46.974955  -92.964824   \n",
       "226321  7545  -93.982767   5964.262632   5463.237368  35.582688 -273.895582   \n",
       "226322  7545   25.564362   2855.431444   2644.568556  55.241678  178.046812   \n",
       "226323  7545    1.320446   1738.853248   1687.646752  51.672052  -18.920813   \n",
       "226324  7545    0.009640    254.495589    239.704411  50.951895  160.990712   \n",
       "...      ...         ...           ...           ...        ...         ...   \n",
       "228295  7610  -77.012660   6470.763074   5799.236926  41.660961 -145.740281   \n",
       "228296  7610   76.026807   7065.827666   5951.672334  51.211188  137.327368   \n",
       "228297  7610  -17.276449   2056.822728   1925.177272  48.258022 -117.575758   \n",
       "228298  7610    0.606521    115.289621    104.410379  49.705775   11.675265   \n",
       "228299  7610  282.995972  24456.262361  22453.737639  55.237626  148.788927   \n",
       "\n",
       "            dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "226320   3.446503    146.266667    148.833333   55.067986  \n",
       "226321  46.098084   5725.833333   5940.000000   55.067986  \n",
       "226322  47.754273   2723.666667   2777.166667   55.067986  \n",
       "226323  34.303349   1706.333333   1696.083333   55.067986  \n",
       "226324  25.369015    247.466667    255.600000   55.067986  \n",
       "...           ...           ...           ...         ...  \n",
       "228295  26.024825   6160.000000   6194.166667    8.296629  \n",
       "228296  27.307389   6465.000000   6712.500000    8.296629  \n",
       "228297  10.393875   2014.666667   1964.333333    8.296629  \n",
       "228298  32.719668    108.366667    110.300000    8.296629  \n",
       "228299  30.400357  23266.666667  23039.166667    8.296629  \n",
       "\n",
       "[1980 rows x 17 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this snippet check what day it is that has the date 2024-09-01 # it is 3570\n",
    "x = processed.copy()\n",
    "x[\"date\"] = pd.to_datetime(processed[\"date\"])\n",
    "target_date = pd.to_datetime(\"2023-11-30\")\n",
    "result = x[x[\"date\"] >= target_date]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b642e8-b5f9-44e5-abe1-4e446285b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day 6591 = 2023-01-02\n",
    "# day 7545 = 2023-11-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4633d9c2-9395-475f-9f9a-fd6a3ad3a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = processed[(6591 <= processed[\"day\"]) & (processed[\"day\"] <= 7545)]\n",
    "# train_processed = processed[processed[\"day\"] <= 7450]\n",
    "test_processed = processed[processed[\"day\"] > 7450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "91bbbb27-66af-40b3-937b-6ac83c804a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data range: 2020-01-02 00:00:00 to 2022-12-30 00:00:00\n",
      "Training data shape: (22050, 18)\n",
      "\n",
      "Testing data range: 2023-01-02 00:00:00 to 2023-12-29 00:00:00\n",
      "Testing data shape: (7170, 18)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_data_based_on_date(df, TRAIN_START_DATE, TRAIN_END_DATE, TEST_START_DATE, TEST_END_DATE):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into training and testing sets based on specified date ranges.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame containing a 'dfdate' column with date information\n",
    "    TRAIN_START_DATE : str\n",
    "        Start date for training data (inclusive)\n",
    "    TRAIN_END_DATE : str\n",
    "        End date for training data (inclusive)\n",
    "    TEST_START_DATE : str\n",
    "        Start date for testing data (inclusive)\n",
    "    TEST_END_DATE : str\n",
    "        End date for testing data (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_df : pandas.DataFrame\n",
    "        DataFrame containing training data\n",
    "    test_df : pandas.DataFrame\n",
    "        DataFrame containing testing data\n",
    "    \"\"\"\n",
    "    # Ensure the dfdate column is in datetime format\n",
    "    df['dfdate'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Create boolean masks for training and testing data\n",
    "    train_mask = (df['dfdate'] >= TRAIN_START_DATE) & (df['dfdate'] <= TRAIN_END_DATE)\n",
    "    test_mask = (df['dfdate'] >= TEST_START_DATE) & (df['dfdate'] <= TEST_END_DATE)\n",
    "    \n",
    "    # Split the data\n",
    "    train_df = df[train_mask].copy()\n",
    "    test_df = df[test_mask].copy()\n",
    "    \n",
    "    # Validate the splits\n",
    "    print(f\"Training data range: {train_df['dfdate'].min()} to {train_df['dfdate'].max()}\")\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"\\nTesting data range: {test_df['dfdate'].min()} to {test_df['dfdate'].max()}\")\n",
    "    print(f\"Testing data shape: {test_df.shape}\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Example usage:\n",
    "TRAIN_START_DATE = '2020-01-01'\n",
    "TRAIN_END_DATE = '2022-12-31'\n",
    "TEST_START_DATE = '2023-01-01'\n",
    "TEST_END_DATE = '2023-12-31'\n",
    "train_processed, test_processed = split_data_based_on_date(processed, TRAIN_START_DATE, TRAIN_END_DATE, TEST_START_DATE, TEST_END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca106ec6-4b7b-4843-8b36-5cb986b1ff25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "      <th>dfdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219751</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>5725.0</td>\n",
       "      <td>5625.0</td>\n",
       "      <td>5725.0</td>\n",
       "      <td>13300500.0</td>\n",
       "      <td>7326</td>\n",
       "      <td>-118.978253</td>\n",
       "      <td>5857.266548</td>\n",
       "      <td>5582.733452</td>\n",
       "      <td>39.372575</td>\n",
       "      <td>-63.890473</td>\n",
       "      <td>25.438537</td>\n",
       "      <td>5861.666667</td>\n",
       "      <td>6156.666667</td>\n",
       "      <td>130.005516</td>\n",
       "      <td>2023-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219781</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>5625.0</td>\n",
       "      <td>5675.0</td>\n",
       "      <td>23044800.0</td>\n",
       "      <td>7327</td>\n",
       "      <td>-114.894161</td>\n",
       "      <td>5828.261065</td>\n",
       "      <td>5589.238935</td>\n",
       "      <td>38.304116</td>\n",
       "      <td>-70.155789</td>\n",
       "      <td>25.438537</td>\n",
       "      <td>5842.500000</td>\n",
       "      <td>6142.083333</td>\n",
       "      <td>39.471353</td>\n",
       "      <td>2023-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219811</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5675.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>5625.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>32580000.0</td>\n",
       "      <td>7328</td>\n",
       "      <td>-108.390736</td>\n",
       "      <td>5826.419836</td>\n",
       "      <td>5586.080164</td>\n",
       "      <td>39.158120</td>\n",
       "      <td>-64.802632</td>\n",
       "      <td>25.438537</td>\n",
       "      <td>5825.000000</td>\n",
       "      <td>6130.833333</td>\n",
       "      <td>39.704552</td>\n",
       "      <td>2023-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219841</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5375.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>5375.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>106047000.0</td>\n",
       "      <td>7329</td>\n",
       "      <td>-125.991863</td>\n",
       "      <td>5882.006803</td>\n",
       "      <td>5500.493197</td>\n",
       "      <td>33.416109</td>\n",
       "      <td>-136.673724</td>\n",
       "      <td>46.585004</td>\n",
       "      <td>5797.500000</td>\n",
       "      <td>6114.583333</td>\n",
       "      <td>54.451649</td>\n",
       "      <td>2023-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219871</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>5525.0</td>\n",
       "      <td>5350.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>59754600.0</td>\n",
       "      <td>7330</td>\n",
       "      <td>-132.363208</td>\n",
       "      <td>5899.209105</td>\n",
       "      <td>5460.790895</td>\n",
       "      <td>35.848926</td>\n",
       "      <td>-152.552927</td>\n",
       "      <td>48.156739</td>\n",
       "      <td>5773.333333</td>\n",
       "      <td>6099.583333</td>\n",
       "      <td>72.995296</td>\n",
       "      <td>2023-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226771</th>\n",
       "      <td>2023-12-21</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5475.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>114749900.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>-42.506357</td>\n",
       "      <td>5804.179416</td>\n",
       "      <td>5445.820584</td>\n",
       "      <td>42.464573</td>\n",
       "      <td>-125.203252</td>\n",
       "      <td>27.878290</td>\n",
       "      <td>5660.833333</td>\n",
       "      <td>5778.750000</td>\n",
       "      <td>36.122380</td>\n",
       "      <td>2023-12-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226801</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>38055800.0</td>\n",
       "      <td>7561</td>\n",
       "      <td>-44.630801</td>\n",
       "      <td>5791.925388</td>\n",
       "      <td>5440.574612</td>\n",
       "      <td>42.464573</td>\n",
       "      <td>-108.969210</td>\n",
       "      <td>27.878290</td>\n",
       "      <td>5653.333333</td>\n",
       "      <td>5767.500000</td>\n",
       "      <td>17.269991</td>\n",
       "      <td>2023-12-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226831</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>5575.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>36424300.0</td>\n",
       "      <td>7562</td>\n",
       "      <td>-45.786638</td>\n",
       "      <td>5790.574497</td>\n",
       "      <td>5434.425503</td>\n",
       "      <td>42.464573</td>\n",
       "      <td>-108.171206</td>\n",
       "      <td>27.878290</td>\n",
       "      <td>5650.000000</td>\n",
       "      <td>5755.416667</td>\n",
       "      <td>25.328176</td>\n",
       "      <td>2023-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226861</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>5575.0</td>\n",
       "      <td>31643200.0</td>\n",
       "      <td>7563</td>\n",
       "      <td>-42.181819</td>\n",
       "      <td>5790.768987</td>\n",
       "      <td>5439.231013</td>\n",
       "      <td>44.227528</td>\n",
       "      <td>-64.885496</td>\n",
       "      <td>23.511463</td>\n",
       "      <td>5647.500000</td>\n",
       "      <td>5745.416667</td>\n",
       "      <td>24.903034</td>\n",
       "      <td>2023-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226891</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>ASII</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>5675.0</td>\n",
       "      <td>5575.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>36245000.0</td>\n",
       "      <td>7564</td>\n",
       "      <td>-34.888222</td>\n",
       "      <td>5792.372455</td>\n",
       "      <td>5450.127545</td>\n",
       "      <td>45.941077</td>\n",
       "      <td>-15.792291</td>\n",
       "      <td>11.320722</td>\n",
       "      <td>5644.166667</td>\n",
       "      <td>5738.333333</td>\n",
       "      <td>27.579128</td>\n",
       "      <td>2023-12-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date   tic   close    high     low    open       volume   day  \\\n",
       "219751  2023-01-02  ASII  5700.0  5725.0  5625.0  5725.0   13300500.0  7326   \n",
       "219781  2023-01-03  ASII  5650.0  5700.0  5625.0  5675.0   23044800.0  7327   \n",
       "219811  2023-01-04  ASII  5675.0  5700.0  5625.0  5650.0   32580000.0  7328   \n",
       "219841  2023-01-05  ASII  5375.0  5700.0  5375.0  5650.0  106047000.0  7329   \n",
       "219871  2023-01-06  ASII  5450.0  5525.0  5350.0  5400.0   59754600.0  7330   \n",
       "...            ...   ...     ...     ...     ...     ...          ...   ...   \n",
       "226771  2023-12-21  ASII  5550.0  5600.0  5475.0  5600.0  114749900.0  7560   \n",
       "226801  2023-12-22  ASII  5550.0  5600.0  5500.0  5550.0   38055800.0  7561   \n",
       "226831  2023-12-27  ASII  5550.0  5575.0  5500.0  5550.0   36424300.0  7562   \n",
       "226861  2023-12-28  ASII  5600.0  5600.0  5550.0  5575.0   31643200.0  7563   \n",
       "226891  2023-12-29  ASII  5650.0  5675.0  5575.0  5600.0   36245000.0  7564   \n",
       "\n",
       "              macd      boll_ub      boll_lb     rsi_30      cci_30  \\\n",
       "219751 -118.978253  5857.266548  5582.733452  39.372575  -63.890473   \n",
       "219781 -114.894161  5828.261065  5589.238935  38.304116  -70.155789   \n",
       "219811 -108.390736  5826.419836  5586.080164  39.158120  -64.802632   \n",
       "219841 -125.991863  5882.006803  5500.493197  33.416109 -136.673724   \n",
       "219871 -132.363208  5899.209105  5460.790895  35.848926 -152.552927   \n",
       "...            ...          ...          ...        ...         ...   \n",
       "226771  -42.506357  5804.179416  5445.820584  42.464573 -125.203252   \n",
       "226801  -44.630801  5791.925388  5440.574612  42.464573 -108.969210   \n",
       "226831  -45.786638  5790.574497  5434.425503  42.464573 -108.171206   \n",
       "226861  -42.181819  5790.768987  5439.231013  44.227528  -64.885496   \n",
       "226891  -34.888222  5792.372455  5450.127545  45.941077  -15.792291   \n",
       "\n",
       "            dx_30  close_30_sma  close_60_sma  turbulence     dfdate  \n",
       "219751  25.438537   5861.666667   6156.666667  130.005516 2023-01-02  \n",
       "219781  25.438537   5842.500000   6142.083333   39.471353 2023-01-03  \n",
       "219811  25.438537   5825.000000   6130.833333   39.704552 2023-01-04  \n",
       "219841  46.585004   5797.500000   6114.583333   54.451649 2023-01-05  \n",
       "219871  48.156739   5773.333333   6099.583333   72.995296 2023-01-06  \n",
       "...           ...           ...           ...         ...        ...  \n",
       "226771  27.878290   5660.833333   5778.750000   36.122380 2023-12-21  \n",
       "226801  27.878290   5653.333333   5767.500000   17.269991 2023-12-22  \n",
       "226831  27.878290   5650.000000   5755.416667   25.328176 2023-12-27  \n",
       "226861  23.511463   5647.500000   5745.416667   24.903034 2023-12-28  \n",
       "226891  11.320722   5644.166667   5738.333333   27.579128 2023-12-29  \n",
       "\n",
       "[239 rows x 18 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traded_tickers = [\"ASII\", \"BDMN\", \"BNGA\", \"BNII\", \"CPIN\", \"DILD\", \"HMSP\", \"UNSP\"]\n",
    "traded_tickers = [\"ASII\"]\n",
    "tmp = test_processed[test_processed[\"tic\"].isin(traded_tickers)]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d14637c-cfb8-4bce-a96e-cc7a16714d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total all days in dataset 7610\n",
      "total all days in train data 735\n",
      "total all days in test data 239\n"
     ]
    }
   ],
   "source": [
    "days = processed[\"day\"].unique()\n",
    "print(f\"total all days in dataset {len(days)}\")\n",
    "days = train_processed[\"day\"].unique()\n",
    "print(f\"total all days in train data {len(days)}\")\n",
    "days = test_processed[\"day\"].unique()\n",
    "print(f\"total all days in test data {len(days)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83122ee9-6a7e-4c43-a4a1-7f928e166d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_array, tech_array, turbulence_array, tickers = df_to_array(train_processed, INDICATORS)\n",
    "\n",
    "env_config = {\n",
    "    \"price_array\": price_array,\n",
    "    \"tech_array\": tech_array,\n",
    "    \"turbulence_array\": turbulence_array,\n",
    "    \"if_train\": True,\n",
    "}\n",
    "env_instance = StockTradingEnv(config=env_config)\n",
    "ERL_PARAMS = {\"learning_rate\": 3e-6,\"batch_size\": 2048,\"gamma\":  0.985,\n",
    "        \"seed\":312,\"net_dimension\":[256,128], \"target_step\":5000, \"eval_gap\":30,\n",
    "        \"eval_times\":1} \n",
    "\n",
    "# read parameters\n",
    "model_name = \"ppo\"\n",
    "cwd = f\"gavin_data_erl/{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7eccb98c-4a09-493e-a6ae-f802085dab15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd487aaa-9997-4439-9ffa-ce4cd811b19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
      "| `time`: Time spent from the start of training to this moment.\n",
      "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `avgS`: Average of steps in an episode.\n",
      "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
      "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
      "|     step      time  |     avgR    stdR    avgS  |     objC      objA\n",
      "| 2.00e+04        30  |  -307.59    0.00     734  |    12.89      0.44\n",
      "| 4.00e+04        61  |  -275.12    0.00     734  |    14.93      0.43\n",
      "| 6.00e+04        91  |  -239.79    0.00     734  |    13.01      0.43\n",
      "| 8.00e+04       121  |  -275.75    0.00     734  |    13.39      0.44\n",
      "| 1.00e+05       152  |  -214.06    0.00     734  |    16.01      0.42\n",
      "| 1.20e+05       182  |  -259.80    0.00     734  |    16.01      0.44\n",
      "| 1.40e+05       213  |  -230.67    0.00     734  |    15.05      0.44\n",
      "| 1.60e+05       243  |  -271.05    0.00     734  |    13.60      0.42\n",
      "| 1.80e+05       273  |  -304.27    0.00     734  |    15.48      0.41\n",
      "| 2.00e+05       304  |  -194.25    0.00     734  |    13.94      0.43\n",
      "| 2.20e+05       334  |  -224.03    0.00     734  |    15.42      0.43\n",
      "| 2.40e+05       365  |  -263.27    0.00     734  |    13.16      0.43\n",
      "| 2.60e+05       396  |  -266.72    0.00     734  |    15.03      0.43\n",
      "| 2.80e+05       426  |  -305.30    0.00     734  |    15.38      0.43\n",
      "| 3.00e+05       457  |  -242.82    0.00     734  |    15.38      0.42\n",
      "| 3.20e+05       487  |  -284.04    0.00     734  |    13.80      0.43\n",
      "| 3.40e+05       518  |  -191.79    0.00     734  |    15.46      0.45\n",
      "| 3.60e+05       549  |  -208.14    0.00     734  |    15.46      0.44\n",
      "| 3.80e+05       579  |  -283.70    0.00     734  |    13.53      0.44\n",
      "| 4.00e+05       609  |  -302.10    0.00     734  |    12.59      0.44\n",
      "| 4.20e+05       640  |  -285.18    0.00     734  |    15.10      0.43\n",
      "| 4.40e+05       670  |  -235.66    0.00     734  |    17.98      0.43\n",
      "| 4.60e+05       701  |  -188.51    0.00     734  |    16.82      0.42\n",
      "| 4.80e+05       731  |  -190.52    0.00     734  |    14.63      0.43\n",
      "| 5.00e+05       762  |  -261.12    0.00     734  |    14.79      0.43\n"
     ]
    }
   ],
   "source": [
    "drl_lib = \"elegantrl\"\n",
    "if drl_lib == \"elegantrl\":\n",
    "    DRLAgent_erl = DRLAgent\n",
    "    break_step = 5e5\n",
    "    agent = DRLAgent(\n",
    "        env=StockTradingEnv,\n",
    "        price_array=price_array,\n",
    "        tech_array=tech_array,\n",
    "        turbulence_array=turbulence_array,\n",
    "    )\n",
    "    model = agent.get_model(model_name, model_kwargs=ERL_PARAMS)\n",
    "    trained_model = agent.train_model(\n",
    "        model=model, cwd=cwd, total_timesteps=break_step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bec800b2-ae07-42ad-b894-4b62cb1fea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_array: 239 days\n"
     ]
    }
   ],
   "source": [
    "test_price_array, test_tech_array, test_turbulence_array, test_tickers, test_dates = df_to_array(test_processed, INDICATORS)\n",
    "env_config = {\n",
    "    \"price_array\": test_price_array,\n",
    "    \"tech_array\": test_tech_array,\n",
    "    \"turbulence_array\": test_turbulence_array,\n",
    "    \"if_train\": False,\n",
    "}\n",
    "env_instance = StockTradingEnv(config=env_config)\n",
    "\n",
    "# load elegantrl needs state dim, action dim and net dim\n",
    "net_dimension = [256, 128] # ERL_PARAMS.get(\"net_dimension\", 2**7)\n",
    "print(f\"price_array: {len(test_price_array)} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1428a52b-d400-467e-bef3-01095b7123ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADMG' 'ASII' 'BDMN' 'BNGA' 'BNII' 'BNLI' 'BRPT' 'CMNP' 'CPIN' 'CTRA']\n"
     ]
    }
   ],
   "source": [
    "print(test_tickers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "366dee01-30dd-4e97-8449-cfdeca1edda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06', '2023-01-09', '2023-01-10', '2023-01-11', '2023-01-12', '2023-01-13']\n"
     ]
    }
   ],
   "source": [
    "print(test_dates[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0561d20b-ec11-4918-bcfe-c5a52367a1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| load actor from: gavin_data_erl/ppo/actor.pth\n",
      "Test Finished!\n",
      "episode_return 0.9665108040000012\n",
      "EPISODE TOTAL ASSETS: [1000000.0, 999001.0090000001, 994131.0090000001, 972616.0090000001, 989477.0090000001, 995064.0090000001, 993183.0090000001, 979810.0090000001, 993860.0090000001, 1002251.0090000001, 1015809.0090000001, 1007945.0090000001, 1016860.0090000001, 1031765.0090000001, 1031880.0090000001, 1035871.0090000001, 1032602.0090000001, 1032301.0090000001, 1024401.0090000001, 1026336.0090000001, 1035060.0090000001, 1039919.0090000001, 1030760.0090000001, 1025648.0090000001, 1022940.0090000001, 1029709.0090000001, 1017827.0090000001, 1009844.0090000001, 1013887.0090000001, 1010440.0090000001, 1014526.0090000001, 1009192.0090000001, 1011234.0090000001, 1009583.0090000001, 1009676.0090000001, 1004448.0090000001, 1000195.0090000001, 1002032.0090000001, 1007184.0090000001, 1011158.0090000001, 1024412.0090000001, 1041808.0090000001, 1032150.0090000001, 1013136.0090000001, 1012850.0090000001, 993658.0090000001, 995982.0090000001, 996303.0090000001, 987850.0090000001, 984157.0090000001, 966689.0090000001, 968232.0090000001, 960958.0090000001, 973372.0090000001, 972201.0090000001, 974601.0090000001, 993390.0090000001, 982769.0090000001, 984348.0090000001, 995782.0090000001, 991127.0090000001, 995013.0090000001, 988243.0090000001, 983904.0090000001, 972583.0090000001, 958456.0090000001, 968782.0090000001, 976150.0090000001, 992868.0090000001, 994713.0090000001, 987067.0090000001, 992992.0090000001, 1008228.0090000001, 1030446.5410000001, 1029417.1910000001, 1032923.1910000001, 1022994.2400000001, 1022994.2400000001, 1022994.2400000001, 1021972.3200000002, 1024059.3200000002, 1040675.3200000002, 1040601.3200000002, 1030778.3200000002, 1035230.3200000002, 1056576.3200000003, 1050443.3200000003, 1051398.3200000003, 1068341.9610000004, 1067274.7920000006, 1058874.7920000006, 1064092.7920000006, 1057846.7920000006, 1060346.7920000006, 1061702.7920000006, 1048058.7920000005, 1040499.6880000005, 1071132.6880000005, 1073323.6880000005, 1076276.6880000005, 1078059.6880000005, 1080099.6880000005, 1089411.6880000005, 1089887.6880000005, 1100052.6880000005, 1100032.6880000005, 1090568.0300000005, 1089478.6380000007, 1087524.6380000007, 1100989.6380000007, 1087549.6380000007, 1084499.6380000007, 1100046.6380000007, 1097473.6380000007, 1116906.6380000007, 1115466.6380000007, 1116294.3130000008, 1116294.3130000008, 1115179.152000001, 1109945.152000001, 1118549.152000001, 1110252.152000001, 1110434.152000001, 1107597.152000001, 1100034.152000001, 1085631.152000001, 1081417.152000001, 1075788.152000001, 1095249.152000001, 1084861.152000001, 1091475.152000001, 1085654.152000001, 1091086.152000001, 1114831.223000001, 1114831.223000001, 1113717.556000001, 1113270.556000001, 1112071.556000001, 1134008.556000001, 1130665.556000001, 1133231.556000001, 1119891.556000001, 1118051.556000001, 1122888.556000001, 1116038.556000001, 1114750.556000001, 1092057.556000001, 1081811.556000001, 1085398.556000001, 1088492.556000001, 1078730.556000001, 1074131.556000001, 1072272.556000001, 1077445.556000001, 1080017.556000001, 1076358.556000001, 1086480.556000001, 1088718.556000001, 1090927.556000001, 1086265.556000001, 1072330.556000001, 1066968.556000001, 1059427.556000001, 1047064.556000001, 1046475.556000001, 1050864.556000001, 1057629.556000001, 1052743.556000001, 1057579.556000001, 1052098.556000001, 1042648.556000001, 1043870.556000001, 1049373.556000001, 1048177.556000001, 1043256.556000001, 1050389.556000001, 1055720.556000001, 1046779.556000001, 1029856.556000001, 1026449.556000001, 1028058.556000001, 1036799.556000001, 1030328.556000001, 1032122.556000001, 1017255.556000001, 1018647.556000001, 999760.556000001, 1003858.556000001, 997832.556000001, 983105.556000001, 981955.556000001, 962246.556000001, 984795.556000001, 991425.556000001, 979412.556000001, 986605.014000001, 986605.014000001, 986605.014000001, 986605.014000001, 985619.4200000012, 1001060.4200000012, 1001706.4200000012, 992181.4200000012, 982863.4200000012, 984225.4200000012, 984247.4200000012, 973586.4200000012, 972740.4200000012, 981604.4200000012, 988478.4200000012, 980330.4200000012, 987971.4200000012, 985794.4200000012, 975258.4200000012, 978694.4200000012, 980287.4200000012, 970858.4200000012, 964217.3140000012, 956934.3140000012, 948634.3140000012, 979422.3140000012, 980350.2380000012, 978261.4310000011, 976683.8040000012, 968600.8040000012, 965905.8040000012, 957974.8040000012, 953852.8040000012, 948062.8040000012, 964818.8040000012, 965288.8040000012, 959022.8040000012, 963875.8040000012, 963143.8040000012, 953384.8040000012, 953067.8040000012, 953558.8040000012, 960615.8040000012, 966510.8040000012]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1053537/1374752988.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n"
     ]
    }
   ],
   "source": [
    "if drl_lib == \"elegantrl\":\n",
    "    DRLAgent_erl = DRLAgent\n",
    "    episode_total_assets = DRLAgent_erl.DRL_prediction(\n",
    "        model_name=model_name,\n",
    "        cwd=cwd,\n",
    "        net_dimension=net_dimension,\n",
    "        environment=env_instance,\n",
    "    )\n",
    "    print(f\"EPISODE TOTAL ASSETS: {episode_total_assets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "471c9eef-2615-4b20-9760-4c8b1148ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed actions is saved to\n",
      "/home/devmiftahul/trading_model/from_finrl-tutorials_git/processed_by_gavin/detailed_actions.xlsx\n",
      "Number of tickers with at least one buy action: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>turbulence</th>\n",
       "      <th>sell_all</th>\n",
       "      <th>ASII</th>\n",
       "      <th>BDMN</th>\n",
       "      <th>BNGA</th>\n",
       "      <th>BNII</th>\n",
       "      <th>CPIN</th>\n",
       "      <th>DILD</th>\n",
       "      <th>HMSP</th>\n",
       "      <th>UNSP</th>\n",
       "      <th>funds_on_market_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>39.471353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5650, 93, 93)</td>\n",
       "      <td>(2730, -5, 0)</td>\n",
       "      <td>(1165, 97, 97)</td>\n",
       "      <td>(242, 99, 99)</td>\n",
       "      <td>(5775, 98, 58)</td>\n",
       "      <td>(166, 99, 9)</td>\n",
       "      <td>(810, 92, 0)</td>\n",
       "      <td>(134, 99, 1)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>39.704552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5675, 96, 93)</td>\n",
       "      <td>(2650, 0, 0)</td>\n",
       "      <td>(1155, 96, 97)</td>\n",
       "      <td>(238, 99, 99)</td>\n",
       "      <td>(5675, 93, 58)</td>\n",
       "      <td>(163, 99, 9)</td>\n",
       "      <td>(805, 99, 0)</td>\n",
       "      <td>(132, 99, 1)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>3</td>\n",
       "      <td>54.451649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5375, 95, 93)</td>\n",
       "      <td>(2620, -6, 0)</td>\n",
       "      <td>(1125, 96, 97)</td>\n",
       "      <td>(230, 99, 99)</td>\n",
       "      <td>(5850, 92, 58)</td>\n",
       "      <td>(156, 99, 9)</td>\n",
       "      <td>(795, 99, 0)</td>\n",
       "      <td>(132, 99, 1)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>4</td>\n",
       "      <td>72.995296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5450, 95, 93)</td>\n",
       "      <td>(2620, -9, 0)</td>\n",
       "      <td>(1145, 96, 97)</td>\n",
       "      <td>(222, 99, 99)</td>\n",
       "      <td>(6000, 92, 58)</td>\n",
       "      <td>(160, 99, 9)</td>\n",
       "      <td>(840, 99, 0)</td>\n",
       "      <td>(134, 99, 1)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>31.434064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5350, 95, 93)</td>\n",
       "      <td>(2600, -10, 0)</td>\n",
       "      <td>(1145, 96, 97)</td>\n",
       "      <td>(226, 99, 99)</td>\n",
       "      <td>(6250, 93, 58)</td>\n",
       "      <td>(160, 99, 9)</td>\n",
       "      <td>(830, 99, 0)</td>\n",
       "      <td>(125, 99, 1)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2023-12-21</td>\n",
       "      <td>234</td>\n",
       "      <td>36.122380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5550, 91, 116)</td>\n",
       "      <td>(2730, 5, 0)</td>\n",
       "      <td>(1680, 94, 104)</td>\n",
       "      <td>(244, 99, 104)</td>\n",
       "      <td>(5025, 97, 21)</td>\n",
       "      <td>(189, 99, 19)</td>\n",
       "      <td>(875, 97, 0)</td>\n",
       "      <td>(112, 99, 3)</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>235</td>\n",
       "      <td>17.269991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5550, 92, 116)</td>\n",
       "      <td>(2720, 5, 0)</td>\n",
       "      <td>(1680, 94, 104)</td>\n",
       "      <td>(246, 99, 104)</td>\n",
       "      <td>(5000, 97, 21)</td>\n",
       "      <td>(189, 99, 19)</td>\n",
       "      <td>(885, 97, 0)</td>\n",
       "      <td>(112, 99, 3)</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>236</td>\n",
       "      <td>25.328176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5550, 92, 116)</td>\n",
       "      <td>(2720, 9, 0)</td>\n",
       "      <td>(1690, 94, 104)</td>\n",
       "      <td>(244, 99, 104)</td>\n",
       "      <td>(4980, 97, 21)</td>\n",
       "      <td>(193, 99, 19)</td>\n",
       "      <td>(890, 97, 0)</td>\n",
       "      <td>(113, 99, 3)</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>237</td>\n",
       "      <td>24.903034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5600, 92, 116)</td>\n",
       "      <td>(2800, 4, 0)</td>\n",
       "      <td>(1695, 94, 104)</td>\n",
       "      <td>(242, 99, 104)</td>\n",
       "      <td>(5025, 97, 21)</td>\n",
       "      <td>(193, 99, 19)</td>\n",
       "      <td>(905, 97, 0)</td>\n",
       "      <td>(113, 99, 3)</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>238</td>\n",
       "      <td>27.579128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(5650, 92, 116)</td>\n",
       "      <td>(2780, 2, 0)</td>\n",
       "      <td>(1695, 95, 104)</td>\n",
       "      <td>(242, 99, 104)</td>\n",
       "      <td>(5025, 97, 21)</td>\n",
       "      <td>(198, 99, 19)</td>\n",
       "      <td>(895, 97, 0)</td>\n",
       "      <td>(113, 99, 3)</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  day  turbulence  sell_all             ASII            BDMN  \\\n",
       "0    2023-01-03    1   39.471353       0.0   (5650, 93, 93)   (2730, -5, 0)   \n",
       "1    2023-01-04    2   39.704552       0.0   (5675, 96, 93)    (2650, 0, 0)   \n",
       "2    2023-01-05    3   54.451649       0.0   (5375, 95, 93)   (2620, -6, 0)   \n",
       "3    2023-01-06    4   72.995296       0.0   (5450, 95, 93)   (2620, -9, 0)   \n",
       "4    2023-01-09    5   31.434064       0.0   (5350, 95, 93)  (2600, -10, 0)   \n",
       "..          ...  ...         ...       ...              ...             ...   \n",
       "233  2023-12-21  234   36.122380       0.0  (5550, 91, 116)    (2730, 5, 0)   \n",
       "234  2023-12-22  235   17.269991       0.0  (5550, 92, 116)    (2720, 5, 0)   \n",
       "235  2023-12-27  236   25.328176       0.0  (5550, 92, 116)    (2720, 9, 0)   \n",
       "236  2023-12-28  237   24.903034       0.0  (5600, 92, 116)    (2800, 4, 0)   \n",
       "237  2023-12-29  238   27.579128       0.0  (5650, 92, 116)    (2780, 2, 0)   \n",
       "\n",
       "                BNGA            BNII            CPIN           DILD  \\\n",
       "0     (1165, 97, 97)   (242, 99, 99)  (5775, 98, 58)   (166, 99, 9)   \n",
       "1     (1155, 96, 97)   (238, 99, 99)  (5675, 93, 58)   (163, 99, 9)   \n",
       "2     (1125, 96, 97)   (230, 99, 99)  (5850, 92, 58)   (156, 99, 9)   \n",
       "3     (1145, 96, 97)   (222, 99, 99)  (6000, 92, 58)   (160, 99, 9)   \n",
       "4     (1145, 96, 97)   (226, 99, 99)  (6250, 93, 58)   (160, 99, 9)   \n",
       "..               ...             ...             ...            ...   \n",
       "233  (1680, 94, 104)  (244, 99, 104)  (5025, 97, 21)  (189, 99, 19)   \n",
       "234  (1680, 94, 104)  (246, 99, 104)  (5000, 97, 21)  (189, 99, 19)   \n",
       "235  (1690, 94, 104)  (244, 99, 104)  (4980, 97, 21)  (193, 99, 19)   \n",
       "236  (1695, 94, 104)  (242, 99, 104)  (5025, 97, 21)  (193, 99, 19)   \n",
       "237  (1695, 95, 104)  (242, 99, 104)  (5025, 97, 21)  (198, 99, 19)   \n",
       "\n",
       "             HMSP          UNSP  funds_on_market_close  \n",
       "0    (810, 92, 0)  (134, 99, 1)                     10  \n",
       "1    (805, 99, 0)  (132, 99, 1)                     10  \n",
       "2    (795, 99, 0)  (132, 99, 1)                     10  \n",
       "3    (840, 99, 0)  (134, 99, 1)                     10  \n",
       "4    (830, 99, 0)  (125, 99, 1)                     10  \n",
       "..            ...           ...                    ...  \n",
       "233  (875, 97, 0)  (112, 99, 3)                     36  \n",
       "234  (885, 97, 0)  (112, 99, 3)                     36  \n",
       "235  (890, 97, 0)  (113, 99, 3)                     36  \n",
       "236  (905, 97, 0)  (113, 99, 3)                     36  \n",
       "237  (895, 97, 0)  (113, 99, 3)                     36  \n",
       "\n",
       "[238 rows x 13 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_action = env_instance.history_action\n",
    "history_amount = env_instance.history_amount\n",
    "turbulence_bool = env_instance.turbulence_bool\n",
    "excel_file = \"/home/devmiftahul/trading_model/from_finrl-tutorials_git/processed_by_gavin/detailed_actions.xlsx\"\n",
    "create_detailed_actions_excel(\n",
    "    excel_file, tickers, history_action, \n",
    "    history_amount, test_turbulence_array, \n",
    "    turbulence_bool, test_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddcf563a-0f2c-4e28-b1e8-3cf948ca8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_detailed_actions_excel(\n",
    "    excel_path: str, tickers: List[str], \n",
    "    history_action: Dict, history_amount: Dict, \n",
    "    turbulence_array: List, turbulence_bool: List,\n",
    "    dates: List\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an Excel file with detailed trading actions for tickers bought at least once.\n",
    "\n",
    "    Parameters:\n",
    "    - tickers: NumPy array or list of ticker symbols\n",
    "    - history_action: Dictionary of actions for each day\n",
    "    - history_amount: Dictionary of amount of money at the end of each market close\n",
    "    - turbulence_array: List of the value of turbulence calculation for each day\n",
    "    - turbulence_bool: List of boolean flag that determines whether we sell all stocks / not for today trade \n",
    "    (if turbulence_array value > turbulence_threshold=99, then turbulence_bool is set to True)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with detailed actions\n",
    "    \"\"\"\n",
    "    # Convert tickers to a list if it's a NumPy array\n",
    "    tickers_list = tickers.tolist() if hasattr(tickers, 'tolist') else list(tickers)\n",
    "\n",
    "    # Find tickers that have been bought at least once\n",
    "    bought_tickers = set()\n",
    "\n",
    "    # Iterate through all days and actions to find bought tickers\n",
    "    for day_actions in history_action.values():\n",
    "        for idx, action_tuple in enumerate(day_actions):\n",
    "            # Check if the stock was bought (positive number of stocks purchased)\n",
    "            if len(action_tuple) >= 3 and action_tuple[1] > 0 and action_tuple[2] > 0:\n",
    "                bought_tickers.add(tickers_list[idx])\n",
    "\n",
    "    # Convert to sorted list for consistent ordering\n",
    "    bought_tickers = sorted(list(bought_tickers))\n",
    "\n",
    "    # Prepare the columns\n",
    "    columns = ['date', 'day', 'turbulence', 'sell_all'] + list(bought_tickers) + ['funds_on_market_close']\n",
    "\n",
    "    # Create a list to store data for each day\n",
    "    data_rows = []\n",
    "\n",
    "    # Iterate through days in the history_action\n",
    "    for day, day_actions in history_action.items():\n",
    "\n",
    "        # Add turbulence value on that day\n",
    "        # turbulence = turbulence_array.get(day, np.nan)\n",
    "        day_index = day\n",
    "        turbulence = turbulence_array[day_index] if day_index < len(turbulence_array) else np.nan\n",
    "\n",
    "        # Add turbulence bool on that day\n",
    "        # sell_all = turbulence_bool.get(day, np.nan)\n",
    "        sell_all = turbulence_bool[day_index] if day_index < len(turbulence_bool) else np.nan\n",
    "\n",
    "        # Create a row for this day\n",
    "        row_data = [dates[day], day, turbulence, sell_all]\n",
    "\n",
    "        # Add actions for bought tickers\n",
    "        day_bought_actions = []\n",
    "        for ticker in bought_tickers:\n",
    "            # Find the index of the ticker in the original tickers list\n",
    "            ticker_index = tickers_list.index(ticker)\n",
    "\n",
    "            # Get the action for this specific ticker\n",
    "            if ticker_index < len(day_actions):\n",
    "                day_bought_actions.append(day_actions[ticker_index])\n",
    "            else:\n",
    "                # Pad with default value if not enough actions\n",
    "                day_bought_actions.append((0, 0, 0))\n",
    "\n",
    "        # Extend row with bought ticker actions\n",
    "        row_data.extend(day_bought_actions)\n",
    "\n",
    "        # Add amount of money at market close\n",
    "        row_data.append(history_amount.get(day, np.nan))\n",
    "\n",
    "        # Append the row to data\n",
    "        data_rows.append(row_data)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows, columns=columns)\n",
    "\n",
    "    # Save to Excel\n",
    "    df.to_excel(excel_file, sheet_name='detailed_actions', index=False)\n",
    "\n",
    "    print(f\"Detailed actions is saved to\\n{excel_file}\")\n",
    "    print(f\"Number of tickers with at least one buy action: {len(bought_tickers)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# # Call the function with the provided variables\n",
    "# # Assuming tickers, history_action, and history_amount are already defined\n",
    "# result_df = create_detailed_actions_excel(tickers, history_action, history_amount)\n",
    "\n",
    "# # Optional: Display first few rows to verify\n",
    "# print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e889daf-c951-480a-82ef-db9fe01efaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def df_to_array(df, tech_indicator_list):\n",
    "    df = df.copy()\n",
    "    dates = sorted(df[\"date\"].unique())\n",
    "    unique_ticker = df.tic.unique()\n",
    "    if_first_time = True\n",
    "    for tic in unique_ticker:\n",
    "        if if_first_time:\n",
    "            price_array = df[df.tic == tic][[\"close\"]].values\n",
    "            tech_array = df[df.tic == tic][tech_indicator_list].values\n",
    "            turbulence_array = df[df.tic == tic][\"turbulence\"].values\n",
    "            if_first_time = False\n",
    "        else:\n",
    "            price_array = np.hstack(\n",
    "                [price_array, df[df.tic == tic][[\"close\"]].values]\n",
    "            )\n",
    "            tech_array = np.hstack(\n",
    "                [tech_array, df[df.tic == tic][tech_indicator_list].values]\n",
    "            )\n",
    "    tech_nan_positions = np.isnan(tech_array)\n",
    "    tech_array[tech_nan_positions] = 0\n",
    "    tech_inf_positions = np.isinf(tech_array)\n",
    "    tech_array[tech_inf_positions] = 0\n",
    "    return price_array, tech_array, turbulence_array, unique_ticker, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e15e190f-4147-4c54-a953-03962b98a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "# from elegantrl.agents import AgentA2C\n",
    "\n",
    "# from actor_ppo import (\n",
    "#     AgentPPO,\n",
    "#     Config,\n",
    "#     train_agent,\n",
    "# )\n",
    "\n",
    "MODELS = {\"ppo\": AgentPPO}\n",
    "OFF_POLICY_MODELS = [\"ddpg\", \"td3\", \"sac\"]\n",
    "ON_POLICY_MODELS = [\"ppo\"]\n",
    "# MODEL_KWARGS = {x: config.__dict__[f\"{x.upper()}_PARAMS\"] for x in MODELS.keys()}\n",
    "#\n",
    "# NOISE = {\n",
    "#     \"normal\": NormalActionNoise,\n",
    "#     \"ornstein_uhlenbeck\": OrnsteinUhlenbeckActionNoise,\n",
    "# }\n",
    "\n",
    "\n",
    "class DRLAgent:\n",
    "    \"\"\"Implementations of DRL algorithms\n",
    "    Attributes\n",
    "    ----------\n",
    "        env: gym environment class\n",
    "            user-defined class\n",
    "    Methods\n",
    "    -------\n",
    "        get_model()\n",
    "            setup DRL algorithms\n",
    "        train_model()\n",
    "            train DRL algorithms in a train dataset\n",
    "            and output the trained model\n",
    "        DRL_prediction()\n",
    "            make a prediction in a test dataset and get results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, price_array, tech_array, turbulence_array):\n",
    "        self.env = env\n",
    "        self.price_array = price_array\n",
    "        self.tech_array = tech_array\n",
    "        self.turbulence_array = turbulence_array\n",
    "\n",
    "    def get_model(self, model_name, model_kwargs):\n",
    "        env_config = {\n",
    "            \"price_array\": self.price_array,\n",
    "            \"tech_array\": self.tech_array,\n",
    "            \"turbulence_array\": self.turbulence_array,\n",
    "            \"if_train\": True,\n",
    "        }\n",
    "        environment = self.env(config=env_config)\n",
    "        env_args = {'config': env_config,\n",
    "              'env_name': environment.env_name,\n",
    "              'state_dim': environment.state_dim,\n",
    "              'action_dim': environment.action_dim,\n",
    "              'if_discrete': False}\n",
    "        agent = MODELS[model_name]\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "        model = Config(agent_class=agent, env_class=self.env, env_args=env_args)\n",
    "        model.if_off_policy = model_name in OFF_POLICY_MODELS\n",
    "        if model_kwargs is not None:\n",
    "            try:\n",
    "                model.learning_rate = model_kwargs[\"learning_rate\"]\n",
    "                model.batch_size = model_kwargs[\"batch_size\"]\n",
    "                model.gamma = model_kwargs[\"gamma\"]\n",
    "                model.seed = model_kwargs[\"seed\"]\n",
    "                model.net_dims = model_kwargs[\"net_dimension\"]\n",
    "                model.target_step = model_kwargs[\"target_step\"]\n",
    "                model.eval_gap = model_kwargs[\"eval_gap\"]\n",
    "                model.eval_times = model_kwargs[\"eval_times\"]\n",
    "            except BaseException:\n",
    "                raise ValueError(\n",
    "                    \"Fail to read arguments, please check 'model_kwargs' input.\"\n",
    "                )\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, cwd, total_timesteps=5000):\n",
    "        model.cwd = cwd\n",
    "        model.break_step = total_timesteps\n",
    "        train_agent(model)\n",
    "\n",
    "    @staticmethod\n",
    "    def DRL_prediction(model_name, cwd, net_dimension, environment):\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "        agent_class = MODELS[model_name]\n",
    "        environment.env_num = 1\n",
    "        agent = agent_class(net_dimension, environment.state_dim, environment.action_dim)\n",
    "        actor = agent.act\n",
    "        # load agent\n",
    "        try:\n",
    "            cwd = cwd + '/actor.pth'\n",
    "            print(f\"| load actor from: {cwd}\")\n",
    "            actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n",
    "            act = actor\n",
    "            device = agent.device\n",
    "        except BaseException:\n",
    "            raise ValueError(\"Fail to load agent!\")\n",
    "\n",
    "        # test on the testing env\n",
    "        _torch = torch\n",
    "        state, _ = environment.reset()\n",
    "        episode_returns = []  # the cumulative_return / initial_account\n",
    "        episode_total_assets = [environment.initial_total_asset]\n",
    "        with _torch.no_grad():\n",
    "            for i in range(environment.max_step):\n",
    "                s_tensor = _torch.as_tensor((state,), device=device)\n",
    "                a_tensor = act(s_tensor)  # action_tanh = act.forward()\n",
    "                action = (\n",
    "                    a_tensor.detach().cpu().numpy()[0]\n",
    "                )  # not need detach(), because with torch.no_grad() outside\n",
    "                state, reward, done, _, _ = environment.step(action)\n",
    "\n",
    "                total_asset = (\n",
    "                    environment.amount\n",
    "                    + (\n",
    "                        environment.price_ary[environment.day] * environment.stocks\n",
    "                    ).sum()\n",
    "                )\n",
    "                episode_total_assets.append(total_asset)\n",
    "                episode_return = total_asset / environment.initial_total_asset\n",
    "                episode_returns.append(episode_return)\n",
    "                if done:\n",
    "                    break\n",
    "        print(\"Test Finished!\")\n",
    "        # return episode total_assets on testing data\n",
    "        print(\"episode_return\", episode_return)\n",
    "        return episode_total_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506550d2-427d-45c8-bf8a-45a4958d5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from torch import Tensor\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "\n",
    "class ActorPPO(nn.Module):\n",
    "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n",
    "        self.action_std_log = nn.Parameter(torch.zeros((1, action_dim)), requires_grad=True)  # trainable parameter\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        return self.net(state).tanh()  # action.tanh()\n",
    "\n",
    "    def get_action(self, state: Tensor) -> (Tensor, Tensor):  # for exploration\n",
    "        action_avg = self.net(state)\n",
    "        action_std = self.action_std_log.exp()\n",
    "\n",
    "        dist = Normal(action_avg, action_std)\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action).sum(1)\n",
    "        return action, logprob\n",
    "\n",
    "    def get_logprob_entropy(self, state: Tensor, action: Tensor) -> (Tensor, Tensor):\n",
    "        action_avg = self.net(state)\n",
    "        action_std = self.action_std_log.exp()\n",
    "\n",
    "        dist = Normal(action_avg, action_std)\n",
    "        logprob = dist.log_prob(action).sum(1)\n",
    "        entropy = dist.entropy().sum(1)\n",
    "        return logprob, entropy\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_action_for_env(action: Tensor) -> Tensor:\n",
    "        return action.tanh()\n",
    "\n",
    "\n",
    "class CriticPPO(nn.Module):\n",
    "    def __init__(self, dims: [int], state_dim: int, _action_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, 1])\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        return self.net(state)  # advantage value\n",
    "\n",
    "\n",
    "def build_mlp(dims: [int]) -> nn.Sequential:  # MLP (MultiLayer Perceptron)\n",
    "    net_list = []\n",
    "    for i in range(len(dims) - 1):\n",
    "        net_list.extend([nn.Linear(dims[i], dims[i + 1]), nn.ReLU()])\n",
    "    del net_list[-1]  # remove the activation of output layer\n",
    "    return nn.Sequential(*net_list)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, agent_class=None, env_class=None, env_args=None):\n",
    "        self.env_class = env_class  # env = env_class(**env_args)\n",
    "        self.env_args = env_args  # env = env_class(**env_args)\n",
    "\n",
    "        if env_args is None:  # dummy env_args\n",
    "            env_args = {'env_name': None, 'state_dim': None, 'action_dim': None, 'if_discrete': None}\n",
    "        self.env_name = env_args['env_name']  # the name of environment. Be used to set 'cwd'.\n",
    "        self.state_dim = env_args['state_dim']  # vector dimension (feature number) of state\n",
    "        self.action_dim = env_args['action_dim']  # vector dimension (feature number) of action\n",
    "        self.if_discrete = env_args['if_discrete']  # discrete or continuous action space\n",
    "\n",
    "        self.agent_class = agent_class  # agent = agent_class(...)\n",
    "\n",
    "        '''Arguments for reward shaping'''\n",
    "        self.gamma = 0.99  # discount factor of future rewards\n",
    "        self.reward_scale = 1.0  # an approximate target reward usually be closed to 256\n",
    "\n",
    "        '''Arguments for training'''\n",
    "        self.gpu_id = int(0)  # `int` means the ID of single GPU, -1 means CPU\n",
    "        self.net_dims = (64, 32)  # the middle layer dimension of MLP (MultiLayer Perceptron)\n",
    "        self.learning_rate = 6e-5  # 2 ** -14 ~= 6e-5\n",
    "        self.soft_update_tau = 5e-3  # 2 ** -8 ~= 5e-3\n",
    "        self.batch_size = int(128)  # num of transitions sampled from replay buffer.\n",
    "        self.horizon_len = int(2000)  # collect horizon_len step while exploring, then update network\n",
    "        self.buffer_size = None  # ReplayBuffer size. Empty the ReplayBuffer for on-policy.\n",
    "        self.repeat_times = 8.0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "\n",
    "        '''Arguments for evaluate'''\n",
    "        self.cwd = None  # current working directory to save model. None means set automatically\n",
    "        self.break_step = +np.inf  # break training if 'total_step > break_step'\n",
    "        self.eval_times = int(32)  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = int(2e4)  # evaluate the agent per training steps\n",
    "\n",
    "    def init_before_training(self):\n",
    "        if self.cwd is None:  # set cwd (current working directory) for saving model\n",
    "            self.cwd = f'./{self.env_name}_{self.agent_class.__name__[5:]}'\n",
    "        os.makedirs(self.cwd, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_gym_env_args(env, if_print: bool) -> dict:\n",
    "    if {'unwrapped', 'observation_space', 'action_space', 'spec'}.issubset(dir(env)):  # isinstance(env, gym.Env):\n",
    "        env_name = env.unwrapped.spec.id\n",
    "        state_shape = env.observation_space.shape\n",
    "        state_dim = state_shape[0] if len(state_shape) == 1 else state_shape  # sometimes state_dim is a list\n",
    "\n",
    "        if_discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "        if if_discrete:  # make sure it is discrete action space\n",
    "            action_dim = env.action_space.n\n",
    "        elif isinstance(env.action_space, gym.spaces.Box):  # make sure it is continuous action space\n",
    "            action_dim = env.action_space.shape[0]\n",
    "\n",
    "    env_args = {'env_name': env_name, 'state_dim': state_dim, 'action_dim': action_dim, 'if_discrete': if_discrete}\n",
    "    print(f\"env_args = {repr(env_args)}\") if if_print else None\n",
    "    return env_args\n",
    "\n",
    "\n",
    "def kwargs_filter(function, kwargs: dict) -> dict:\n",
    "    import inspect\n",
    "    sign = inspect.signature(function).parameters.values()\n",
    "    sign = {val.name for val in sign}\n",
    "    common_args = sign.intersection(kwargs.keys())\n",
    "    return {key: kwargs[key] for key in common_args}  # filtered kwargs\n",
    "\n",
    "\n",
    "def build_env(env_class=None, env_args=None):\n",
    "    if env_class.__module__ == 'gym.envs.registration':  # special rule\n",
    "        env = env_class(id=env_args['env_name'])\n",
    "    else:\n",
    "        env = env_class(**kwargs_filter(env_class.__init__, env_args.copy()))\n",
    "    for attr_str in ('env_name', 'state_dim', 'action_dim', 'if_discrete'):\n",
    "        setattr(env, attr_str, env_args[attr_str])\n",
    "    return env\n",
    "\n",
    "\n",
    "class AgentBase:\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.gamma = args.gamma\n",
    "        self.batch_size = args.batch_size\n",
    "        self.repeat_times = args.repeat_times\n",
    "        self.reward_scale = args.reward_scale\n",
    "        self.soft_update_tau = args.soft_update_tau\n",
    "\n",
    "        self.states = None  # assert self.states == (1, state_dim)\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        act_class = getattr(self, \"act_class\", None)\n",
    "        cri_class = getattr(self, \"cri_class\", None)\n",
    "        self.act = self.act_target = act_class(net_dims, state_dim, action_dim).to(self.device)\n",
    "        self.cri = self.cri_target = cri_class(net_dims, state_dim, action_dim).to(self.device) \\\n",
    "            if cri_class else self.act\n",
    "\n",
    "        self.act_optimizer = torch.optim.Adam(self.act.parameters(), args.learning_rate)\n",
    "        self.cri_optimizer = torch.optim.Adam(self.cri.parameters(), args.learning_rate) \\\n",
    "            if cri_class else self.act_optimizer\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer_update(optimizer, objective: Tensor):\n",
    "        optimizer.zero_grad()\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(target_net: torch.nn.Module, current_net: torch.nn.Module, tau: float):\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
    "\n",
    "\n",
    "class AgentPPO(AgentBase):\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.if_off_policy = False\n",
    "        self.act_class = getattr(self, \"act_class\", ActorPPO)\n",
    "        self.cri_class = getattr(self, \"cri_class\", CriticPPO)\n",
    "        AgentBase.__init__(self, net_dims, state_dim, action_dim, gpu_id, args)\n",
    "\n",
    "        self.ratio_clip = getattr(args, \"ratio_clip\", 0.25)  # `ratio.clamp(1 - clip, 1 + clip)`\n",
    "        self.lambda_gae_adv = getattr(args, \"lambda_gae_adv\", 0.95)  # could be 0.80~0.99\n",
    "        self.lambda_entropy = getattr(args, \"lambda_entropy\", 0.01)  # could be 0.00~0.10\n",
    "        self.lambda_entropy = torch.tensor(self.lambda_entropy, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def explore_env(self, env, horizon_len: int) -> [Tensor]:\n",
    "        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.action_dim), dtype=torch.float32).to(self.device)\n",
    "        logprobs = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros(horizon_len, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        ary_state = self.states[0]\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        convert = self.act.convert_action_for_env\n",
    "        for i in range(horizon_len):\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n",
    "            action, logprob = [t.squeeze(0) for t in get_action(state.unsqueeze(0))[:2]]\n",
    "\n",
    "            ary_action = convert(action).detach().cpu().numpy()\n",
    "            ary_state, reward, done, _, _ = env.step(ary_action)\n",
    "            if done:\n",
    "                ary_state, _ = env.reset()\n",
    "\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            logprobs[i] = logprob\n",
    "            rewards[i] = reward\n",
    "            dones[i] = done\n",
    "\n",
    "        self.states[0] = ary_state\n",
    "        rewards = (rewards * self.reward_scale).unsqueeze(1)\n",
    "        undones = (1 - dones.type(torch.float32)).unsqueeze(1)\n",
    "        return states, actions, logprobs, rewards, undones\n",
    "\n",
    "    def update_net(self, buffer) -> [float]:\n",
    "        with torch.no_grad():\n",
    "            states, actions, logprobs, rewards, undones = buffer\n",
    "            buffer_size = states.shape[0]\n",
    "\n",
    "            '''get advantages reward_sums'''\n",
    "            bs = 2 ** 10  # set a smaller 'batch_size' when out of GPU memory.\n",
    "            values = [self.cri(states[i:i + bs]) for i in range(0, buffer_size, bs)]\n",
    "            values = torch.cat(values, dim=0).squeeze(1)  # values.shape == (buffer_size, )\n",
    "\n",
    "            advantages = self.get_advantages(rewards, undones, values)  # advantages.shape == (buffer_size, )\n",
    "            reward_sums = advantages + values  # reward_sums.shape == (buffer_size, )\n",
    "            del rewards, undones, values\n",
    "\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std(dim=0) + 1e-5)\n",
    "        assert logprobs.shape == advantages.shape == reward_sums.shape == (buffer_size,)\n",
    "        '''update network'''\n",
    "        obj_critics = 0.0\n",
    "        obj_actors = 0.0\n",
    "\n",
    "        update_times = int(buffer_size * self.repeat_times / self.batch_size)\n",
    "        assert update_times >= 1\n",
    "        for _ in range(update_times):\n",
    "            indices = torch.randint(buffer_size, size=(self.batch_size,), requires_grad=False)\n",
    "            state = states[indices]\n",
    "            action = actions[indices]\n",
    "            logprob = logprobs[indices]\n",
    "            advantage = advantages[indices]\n",
    "            reward_sum = reward_sums[indices]\n",
    "\n",
    "            value = self.cri(state).squeeze(1)  # critic network predicts the reward_sum (Q value) of state\n",
    "            obj_critic = self.criterion(value, reward_sum)\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "\n",
    "            new_logprob, obj_entropy = self.act.get_logprob_entropy(state, action)\n",
    "            ratio = (new_logprob - logprob.detach()).exp()\n",
    "            surrogate1 = advantage * ratio\n",
    "            surrogate2 = advantage * ratio.clamp(1 - self.ratio_clip, 1 + self.ratio_clip)\n",
    "            obj_surrogate = torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            obj_actor = obj_surrogate + obj_entropy.mean() * self.lambda_entropy\n",
    "            self.optimizer_update(self.act_optimizer, -obj_actor)\n",
    "\n",
    "            obj_critics += obj_critic.item()\n",
    "            obj_actors += obj_actor.item()\n",
    "        a_std_log = getattr(self.act, 'a_std_log', torch.zeros(1)).mean()\n",
    "        return obj_critics / update_times, obj_actors / update_times, a_std_log.item()\n",
    "\n",
    "    def get_advantages(self, rewards: Tensor, undones: Tensor, values: Tensor) -> Tensor:\n",
    "        advantages = torch.empty_like(values)  # advantage value\n",
    "\n",
    "        masks = undones * self.gamma\n",
    "        horizon_len = rewards.shape[0]\n",
    "\n",
    "        next_state = torch.tensor(self.states, dtype=torch.float32).to(self.device)\n",
    "        next_value = self.cri(next_state).detach()[0, 0]\n",
    "\n",
    "        advantage = 0  # last_gae_lambda\n",
    "        for t in range(horizon_len - 1, -1, -1):\n",
    "            delta = rewards[t] + masks[t] * next_value - values[t]\n",
    "            advantages[t] = advantage = delta + masks[t] * self.lambda_gae_adv * advantage\n",
    "            next_value = values[t]\n",
    "        return advantages\n",
    "\n",
    "class PendulumEnv(gym.Wrapper):  # a demo of custom gym env\n",
    "    def __init__(self):\n",
    "        gym.logger.set_level(40)  # Block warning\n",
    "        gym_env_name = \"Pendulum-v0\" if gym.__version__ < '0.18.0' else \"Pendulum-v1\"\n",
    "        super().__init__(env=gym.make(gym_env_name))\n",
    "\n",
    "        '''the necessary env information when you design a custom env'''\n",
    "        self.env_name = gym_env_name  # the name of this env.\n",
    "        self.state_dim = self.observation_space.shape[0]  # feature number of state\n",
    "        self.action_dim = self.action_space.shape[0]  # feature number of action\n",
    "        self.if_discrete = False  # discrete action or continuous action\n",
    "\n",
    "    def reset(self) -> np.ndarray:  # reset the agent in env\n",
    "        resetted_env, _ = self.env.reset()\n",
    "        return resetted_env\n",
    "\n",
    "    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):  # agent interacts in env\n",
    "        # We suggest that adjust action space to (-1, +1) when designing a custom env.\n",
    "        state, reward, done, info_dict, _ = self.env.step(action * 2)\n",
    "        return state.reshape(self.state_dim), float(reward), done, info_dict\n",
    "\n",
    "    \n",
    "def train_agent(args: Config):\n",
    "    args.init_before_training()\n",
    "\n",
    "    env = build_env(args.env_class, args.env_args)\n",
    "    agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n",
    "\n",
    "    new_env, _ = env.reset()\n",
    "    agent.states = new_env[np.newaxis, :]\n",
    "\n",
    "    evaluator = Evaluator(eval_env=build_env(args.env_class, args.env_args),\n",
    "                          eval_per_step=args.eval_per_step,\n",
    "                          eval_times=args.eval_times,\n",
    "                          cwd=args.cwd)\n",
    "    torch.set_grad_enabled(False)\n",
    "    while True: # start training\n",
    "        buffer_items = agent.explore_env(env, args.horizon_len)\n",
    "        torch.set_grad_enabled(True)\n",
    "        logging_tuple = agent.update_net(buffer_items)\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        evaluator.evaluate_and_save(agent.act, args.horizon_len, logging_tuple)\n",
    "        if (evaluator.total_step > args.break_step) or os.path.exists(f\"{args.cwd}/stop\"):\n",
    "            torch.save(agent.act.state_dict(), args.cwd + '/actor.pth')\n",
    "            break  # stop training when reach `break_step` or `mkdir cwd/stop`\n",
    "\n",
    "\n",
    "def render_agent(env_class, env_args: dict, net_dims: [int], agent_class, actor_path: str, render_times: int = 8):\n",
    "    env = build_env(env_class, env_args)\n",
    "\n",
    "    state_dim = env_args['state_dim']\n",
    "    action_dim = env_args['action_dim']\n",
    "    agent = agent_class(net_dims, state_dim, action_dim, gpu_id=-1)\n",
    "    actor = agent.act\n",
    "\n",
    "    print(f\"| render and load actor from: {actor_path}\")\n",
    "    actor.load_state_dict(torch.load(actor_path, map_location=lambda storage, loc: storage))\n",
    "    for i in range(render_times):\n",
    "        cumulative_reward, episode_step = get_rewards_and_steps(env, actor, if_render=True)\n",
    "        print(f\"|{i:4}  cumulative_reward {cumulative_reward:9.3f}  episode_step {episode_step:5.0f}\")\n",
    "\n",
    "        \n",
    "class Evaluator:\n",
    "    def __init__(self, eval_env, eval_per_step: int = 1e4, eval_times: int = 8, cwd: str = '.'):\n",
    "        self.cwd = cwd\n",
    "        self.env_eval = eval_env\n",
    "        self.eval_step = 0\n",
    "        self.total_step = 0\n",
    "        self.start_time = time.time()\n",
    "        self.eval_times = eval_times  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = eval_per_step  # evaluate the agent per training steps\n",
    "\n",
    "        self.recorder = []\n",
    "        print(f\"\\n| `step`: Number of samples, or total training steps, or running times of `env.step()`.\"\n",
    "              f\"\\n| `time`: Time spent from the start of training to this moment.\"\n",
    "              f\"\\n| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              f\"\\n| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              f\"\\n| `avgS`: Average of steps in an episode.\"\n",
    "              f\"\\n| `objC`: Objective of Critic network. Or call it loss function of critic network.\"\n",
    "              f\"\\n| `objA`: Objective of Actor network. It is the average Q value of the critic network.\"\n",
    "              f\"\\n| {'step':>8}  {'time':>8}  | {'avgR':>8}  {'stdR':>6}  {'avgS':>6}  | {'objC':>8}  {'objA':>8}\")\n",
    "            \n",
    "    def evaluate_and_save(self, actor, horizon_len: int, logging_tuple: tuple):\n",
    "        self.total_step += horizon_len\n",
    "        if self.eval_step + self.eval_per_step > self.total_step:\n",
    "            return\n",
    "        self.eval_step = self.total_step\n",
    "\n",
    "        rewards_steps_ary = [get_rewards_and_steps(self.env_eval, actor) for _ in range(self.eval_times)]\n",
    "        rewards_steps_ary = np.array(rewards_steps_ary, dtype=np.float32)\n",
    "        avg_r = rewards_steps_ary[:, 0].mean()  # average of cumulative rewards\n",
    "        std_r = rewards_steps_ary[:, 0].std()  # std of cumulative rewards\n",
    "        avg_s = rewards_steps_ary[:, 1].mean()  # average of steps in an episode\n",
    "\n",
    "        used_time = time.time() - self.start_time\n",
    "        self.recorder.append((self.total_step, used_time, avg_r))\n",
    "        \n",
    "        print(f\"| {self.total_step:8.2e}  {used_time:8.0f}  \"\n",
    "              f\"| {avg_r:8.2f}  {std_r:6.2f}  {avg_s:6.0f}  \"\n",
    "              f\"| {logging_tuple[0]:8.2f}  {logging_tuple[1]:8.2f}\")\n",
    "\n",
    "\n",
    "def get_rewards_and_steps(env, actor, if_render: bool = False) -> (float, int):  # cumulative_rewards and episode_steps\n",
    "    device = next(actor.parameters()).device  # net.parameters() is a Python generator.\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    episode_steps = 0\n",
    "    cumulative_returns = 0.0  # sum of rewards in an episode\n",
    "    for episode_steps in range(12345):\n",
    "        tensor_state = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        tensor_action = actor(tensor_state)\n",
    "        action = tensor_action.detach().cpu().numpy()[0]  # not need detach(), because using torch.no_grad() outside\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        cumulative_returns += reward\n",
    "\n",
    "        if if_render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "    return cumulative_returns, episode_steps + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6217f0-34b4-43aa-996c-5cde03a7f4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
