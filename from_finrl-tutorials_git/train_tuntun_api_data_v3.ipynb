{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd52197-6d04-453f-8089-43a72da272e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# processed_data_fname = \"/home/devmiftahul/trading_model/from_finrl-tutorials_git/tuntun_scripts/processed_data/processed_data_136_tickers.csv\"\n",
    "# dfx = pd.read_csv(processed_data_fname)\n",
    "# tickers = sorted(dfx[\"tic\"].unique())\n",
    "# print(len(tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb01f28f-03b3-43ca-8e5a-f83e1edf8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df = dfx.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf00099f-377c-49f8-8889-2145dfae411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume_zero_count = raw_df[raw_df['volume'] == 1].groupby('tic').size()\n",
    "# sorted_volume_zero_count = volume_zero_count.sort_values(ascending=True)\n",
    "# print(sorted_volume_zero_count)\n",
    "# top_n_tic = sorted_volume_zero_count.head(100)\n",
    "# tickers = top_n_tic.index.tolist()\n",
    "# df = raw_df[raw_df[\"tic\"].isin(tickers)]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d90c28-618d-48c3-bc4e-f8afe65ba591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>151.00</td>\n",
       "      <td>151.00</td>\n",
       "      <td>147.00</td>\n",
       "      <td>148.00</td>\n",
       "      <td>1380000.0</td>\n",
       "      <td>ACES</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>149.914214</td>\n",
       "      <td>147.085786</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>630.00</td>\n",
       "      <td>630.00</td>\n",
       "      <td>620.00</td>\n",
       "      <td>630.00</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>ADES</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>149.914214</td>\n",
       "      <td>147.085786</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>630.000000</td>\n",
       "      <td>630.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>347.87</td>\n",
       "      <td>347.87</td>\n",
       "      <td>343.63</td>\n",
       "      <td>347.87</td>\n",
       "      <td>760195.0</td>\n",
       "      <td>ADHI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>149.914214</td>\n",
       "      <td>147.085786</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>347.870000</td>\n",
       "      <td>347.870000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>6800.00</td>\n",
       "      <td>6800.00</td>\n",
       "      <td>6800.00</td>\n",
       "      <td>6800.00</td>\n",
       "      <td>17500.0</td>\n",
       "      <td>ADMF</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>149.914214</td>\n",
       "      <td>147.085786</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>134.00</td>\n",
       "      <td>142.00</td>\n",
       "      <td>134.00</td>\n",
       "      <td>142.00</td>\n",
       "      <td>20006500.0</td>\n",
       "      <td>ADMG</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>149.914214</td>\n",
       "      <td>147.085786</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362995</th>\n",
       "      <td>2024-11-26</td>\n",
       "      <td>2550.00</td>\n",
       "      <td>2560.00</td>\n",
       "      <td>2510.00</td>\n",
       "      <td>2550.00</td>\n",
       "      <td>1474900.0</td>\n",
       "      <td>TSPC</td>\n",
       "      <td>3630</td>\n",
       "      <td>-21.482045</td>\n",
       "      <td>2718.864030</td>\n",
       "      <td>2547.135970</td>\n",
       "      <td>47.421834</td>\n",
       "      <td>-168.905306</td>\n",
       "      <td>21.868859</td>\n",
       "      <td>2654.333333</td>\n",
       "      <td>2654.666667</td>\n",
       "      <td>249.067994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362996</th>\n",
       "      <td>2024-11-26</td>\n",
       "      <td>1670.00</td>\n",
       "      <td>1685.00</td>\n",
       "      <td>1665.00</td>\n",
       "      <td>1670.00</td>\n",
       "      <td>1319700.0</td>\n",
       "      <td>ULTJ</td>\n",
       "      <td>3630</td>\n",
       "      <td>-28.982196</td>\n",
       "      <td>1780.892823</td>\n",
       "      <td>1665.107177</td>\n",
       "      <td>39.086598</td>\n",
       "      <td>-101.521955</td>\n",
       "      <td>28.780787</td>\n",
       "      <td>1769.000000</td>\n",
       "      <td>1813.500000</td>\n",
       "      <td>249.067994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362997</th>\n",
       "      <td>2024-11-26</td>\n",
       "      <td>123.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>284200.0</td>\n",
       "      <td>UNSP</td>\n",
       "      <td>3630</td>\n",
       "      <td>5.733318</td>\n",
       "      <td>142.897927</td>\n",
       "      <td>103.302073</td>\n",
       "      <td>60.280480</td>\n",
       "      <td>39.975513</td>\n",
       "      <td>21.470936</td>\n",
       "      <td>115.433333</td>\n",
       "      <td>100.516667</td>\n",
       "      <td>249.067994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362998</th>\n",
       "      <td>2024-11-26</td>\n",
       "      <td>129.00</td>\n",
       "      <td>130.00</td>\n",
       "      <td>127.00</td>\n",
       "      <td>129.00</td>\n",
       "      <td>3067800.0</td>\n",
       "      <td>WEHA</td>\n",
       "      <td>3630</td>\n",
       "      <td>-3.676446</td>\n",
       "      <td>146.304369</td>\n",
       "      <td>124.495631</td>\n",
       "      <td>43.942857</td>\n",
       "      <td>-88.149445</td>\n",
       "      <td>25.055273</td>\n",
       "      <td>142.533333</td>\n",
       "      <td>140.466667</td>\n",
       "      <td>249.067994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362999</th>\n",
       "      <td>2024-11-26</td>\n",
       "      <td>324.00</td>\n",
       "      <td>326.00</td>\n",
       "      <td>304.00</td>\n",
       "      <td>324.00</td>\n",
       "      <td>18459500.0</td>\n",
       "      <td>WIKA</td>\n",
       "      <td>3630</td>\n",
       "      <td>-12.015342</td>\n",
       "      <td>404.962565</td>\n",
       "      <td>270.437435</td>\n",
       "      <td>47.145680</td>\n",
       "      <td>-74.050897</td>\n",
       "      <td>13.903473</td>\n",
       "      <td>355.533333</td>\n",
       "      <td>378.666667</td>\n",
       "      <td>249.067994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date     open     high      low    close      volume   tic  \\\n",
       "0       2010-01-04   151.00   151.00   147.00   148.00   1380000.0  ACES   \n",
       "1       2010-01-04   630.00   630.00   620.00   630.00    162000.0  ADES   \n",
       "2       2010-01-04   347.87   347.87   343.63   347.87    760195.0  ADHI   \n",
       "3       2010-01-04  6800.00  6800.00  6800.00  6800.00     17500.0  ADMF   \n",
       "4       2010-01-04   134.00   142.00   134.00   142.00  20006500.0  ADMG   \n",
       "...            ...      ...      ...      ...      ...         ...   ...   \n",
       "362995  2024-11-26  2550.00  2560.00  2510.00  2550.00   1474900.0  TSPC   \n",
       "362996  2024-11-26  1670.00  1685.00  1665.00  1670.00   1319700.0  ULTJ   \n",
       "362997  2024-11-26   123.00   123.00   122.00   123.00    284200.0  UNSP   \n",
       "362998  2024-11-26   129.00   130.00   127.00   129.00   3067800.0  WEHA   \n",
       "362999  2024-11-26   324.00   326.00   304.00   324.00  18459500.0  WIKA   \n",
       "\n",
       "         day       macd      boll_ub      boll_lb      rsi_30      cci_30  \\\n",
       "0          1   0.000000   149.914214   147.085786  100.000000  -66.666667   \n",
       "1          1   0.000000   149.914214   147.085786  100.000000  -66.666667   \n",
       "2          1   0.000000   149.914214   147.085786  100.000000  -66.666667   \n",
       "3          1   0.000000   149.914214   147.085786  100.000000  -66.666667   \n",
       "4          1   0.000000   149.914214   147.085786  100.000000  -66.666667   \n",
       "...      ...        ...          ...          ...         ...         ...   \n",
       "362995  3630 -21.482045  2718.864030  2547.135970   47.421834 -168.905306   \n",
       "362996  3630 -28.982196  1780.892823  1665.107177   39.086598 -101.521955   \n",
       "362997  3630   5.733318   142.897927   103.302073   60.280480   39.975513   \n",
       "362998  3630  -3.676446   146.304369   124.495631   43.942857  -88.149445   \n",
       "362999  3630 -12.015342   404.962565   270.437435   47.145680  -74.050897   \n",
       "\n",
       "             dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "0       100.000000    148.000000    148.000000    0.000000  \n",
       "1       100.000000    630.000000    630.000000    0.000000  \n",
       "2       100.000000    347.870000    347.870000    0.000000  \n",
       "3       100.000000   6800.000000   6800.000000    0.000000  \n",
       "4       100.000000    142.000000    142.000000    0.000000  \n",
       "...            ...           ...           ...         ...  \n",
       "362995   21.868859   2654.333333   2654.666667  249.067994  \n",
       "362996   28.780787   1769.000000   1813.500000  249.067994  \n",
       "362997   21.470936    115.433333    100.516667  249.067994  \n",
       "362998   25.055273    142.533333    140.466667  249.067994  \n",
       "362999   13.903473    355.533333    378.666667  249.067994  \n",
       "\n",
       "[363000 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# DONT USE THIS PATH BECAUSE ALL VOLUME HERE IS 0 (bug in fetch_data_using_api.py)\n",
    "# processed_data_fname = \"/home/devmiftahul/trading_model/from_finrl-tutorials_git/tuntun_scripts/processed_data/processed_data_135_tickers.csv\"\n",
    "\n",
    "features_csv = \"/home/devmiftahul/trading_model/from_finrl-tutorials_git/tuntun_scripts/processed_data/100_tickers_with_features.csv\"\n",
    "processed = pd.read_csv(features_csv)\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fef9c89-df43-4e81-84e0-5bb59686aa53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3630"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_days = processed[\"day\"].unique()\n",
    "len(unique_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c209be5e-7e08-468d-b783-1e1dd143fca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macd',\n",
       " 'boll_ub',\n",
       " 'boll_lb',\n",
       " 'rsi_30',\n",
       " 'cci_30',\n",
       " 'dx_30',\n",
       " 'close_30_sma',\n",
       " 'close_60_sma']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
    "from finrl.config import INDICATORS\n",
    "INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e24c4351-da45-4d05-85ca-c3bcaf296e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = processed[(1000 < processed[\"day\"]) & (processed[\"day\"] <= 3500)]\n",
    "test_processed = processed[processed[\"day\"] > 3500]\n",
    "price_array, tech_array, turbulence_array, tickers = df_to_array(train_processed, INDICATORS)\n",
    "\n",
    "env_config = {\n",
    "    \"price_array\": price_array,\n",
    "    \"tech_array\": tech_array,\n",
    "    \"turbulence_array\": turbulence_array,\n",
    "    \"if_train\": True,\n",
    "}\n",
    "env_instance = StockTradingEnv(config=env_config)\n",
    "ERL_PARAMS = {\"learning_rate\": 3e-6,\"batch_size\": 2048,\"gamma\":  0.985,\n",
    "        \"seed\":312,\"net_dimension\":[128,64], \"target_step\":5000, \"eval_gap\":30,\n",
    "        \"eval_times\":1} \n",
    "\n",
    "# read parameters\n",
    "model_name = \"ppo\"\n",
    "cwd = f\"tuntun_papertrading_erl/{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cec690fa-a554-4fe8-88be-0916dae72ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total all days in dataset 3630\n",
      "total all days in train data 2500\n",
      "total all days in test data 130\n"
     ]
    }
   ],
   "source": [
    "days = processed[\"day\"].unique()\n",
    "print(f\"total all days in dataset {len(days)}\")\n",
    "days = train_processed[\"day\"].unique()\n",
    "print(f\"total all days in train data {len(days)}\")\n",
    "days = test_processed[\"day\"].unique()\n",
    "print(f\"total all days in test data {len(days)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de93190e-df04-41e0-a92a-3c134e9c21c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
      "| `time`: Time spent from the start of training to this moment.\n",
      "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `avgS`: Average of steps in an episode.\n",
      "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
      "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
      "|     step      time  |     avgR    stdR    avgS  |     objC      objA\n",
      "| 2.00e+04        31  |   -90.89    0.00    2499  |     4.78      1.43\n",
      "| 4.00e+04        62  |   -83.60    0.00    2499  |     4.51      1.44\n",
      "| 6.00e+04        94  |   -94.52    0.00    2499  |     4.92      1.42\n",
      "| 8.00e+04       126  |  -100.36    0.00    2499  |     4.74      1.41\n",
      "| 1.00e+05       157  |   -87.79    0.00    2499  |     4.34      1.43\n"
     ]
    }
   ],
   "source": [
    "drl_lib = \"elegantrl\"\n",
    "if drl_lib == \"elegantrl\":\n",
    "    DRLAgent_erl = DRLAgent\n",
    "    break_step = 1e5\n",
    "    agent = DRLAgent(\n",
    "        env=StockTradingEnv,\n",
    "        price_array=price_array,\n",
    "        tech_array=tech_array,\n",
    "        turbulence_array=turbulence_array,\n",
    "    )\n",
    "    model = agent.get_model(model_name, model_kwargs=ERL_PARAMS)\n",
    "    trained_model = agent.train_model(\n",
    "        model=model, cwd=cwd, total_timesteps=break_step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5c6151d-66df-4cd8-957d-3b702431e216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f261b11-dcc1-4fba-9717-41267e60d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_array: 130 days\n",
      "| load actor from: tuntun_papertrading_erl/ppo/actor.pth\n",
      "Test Finished!\n",
      "episode_return 0.992557452\n",
      "EPISODE TOTAL ASSETS: [1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 1000000.0, 999001.001, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452, 992557.452]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_804155/1374752988.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n"
     ]
    }
   ],
   "source": [
    "test_price_array, test_tech_array, test_turbulence_array, test_tickers = df_to_array(test_processed, INDICATORS)\n",
    "env_config = {\n",
    "    \"price_array\": test_price_array,\n",
    "    \"tech_array\": test_tech_array,\n",
    "    \"turbulence_array\": test_turbulence_array,\n",
    "    \"if_train\": False,\n",
    "}\n",
    "env_instance = StockTradingEnv(config=env_config)\n",
    "\n",
    "# load elegantrl needs state dim, action dim and net dim\n",
    "net_dimension = ERL_PARAMS.get(\"net_dimension\", 2**7)\n",
    "print(f\"price_array: {len(test_price_array)} days\")\n",
    "\n",
    "if drl_lib == \"elegantrl\":\n",
    "    DRLAgent_erl = DRLAgent\n",
    "    episode_total_assets = DRLAgent_erl.DRL_prediction(\n",
    "        model_name=model_name,\n",
    "        cwd=cwd,\n",
    "        net_dimension=net_dimension,\n",
    "        environment=env_instance,\n",
    "    )\n",
    "    print(f\"EPISODE TOTAL ASSETS: {episode_total_assets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93a3cd30-2b60-4676-ae06-65167a054d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from torch import Tensor\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "\n",
    "class ActorPPO(nn.Module):\n",
    "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n",
    "        self.action_std_log = nn.Parameter(torch.zeros((1, action_dim)), requires_grad=True)  # trainable parameter\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        return self.net(state).tanh()  # action.tanh()\n",
    "\n",
    "    def get_action(self, state: Tensor) -> (Tensor, Tensor):  # for exploration\n",
    "        action_avg = self.net(state)\n",
    "        action_std = self.action_std_log.exp()\n",
    "\n",
    "        dist = Normal(action_avg, action_std)\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action).sum(1)\n",
    "        return action, logprob\n",
    "\n",
    "    def get_logprob_entropy(self, state: Tensor, action: Tensor) -> (Tensor, Tensor):\n",
    "        action_avg = self.net(state)\n",
    "        action_std = self.action_std_log.exp()\n",
    "\n",
    "        dist = Normal(action_avg, action_std)\n",
    "        logprob = dist.log_prob(action).sum(1)\n",
    "        entropy = dist.entropy().sum(1)\n",
    "        return logprob, entropy\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_action_for_env(action: Tensor) -> Tensor:\n",
    "        return action.tanh()\n",
    "\n",
    "\n",
    "class CriticPPO(nn.Module):\n",
    "    def __init__(self, dims: [int], state_dim: int, _action_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, 1])\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        return self.net(state)  # advantage value\n",
    "\n",
    "\n",
    "def build_mlp(dims: [int]) -> nn.Sequential:  # MLP (MultiLayer Perceptron)\n",
    "    net_list = []\n",
    "    for i in range(len(dims) - 1):\n",
    "        net_list.extend([nn.Linear(dims[i], dims[i + 1]), nn.ReLU()])\n",
    "    del net_list[-1]  # remove the activation of output layer\n",
    "    return nn.Sequential(*net_list)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, agent_class=None, env_class=None, env_args=None):\n",
    "        self.env_class = env_class  # env = env_class(**env_args)\n",
    "        self.env_args = env_args  # env = env_class(**env_args)\n",
    "\n",
    "        if env_args is None:  # dummy env_args\n",
    "            env_args = {'env_name': None, 'state_dim': None, 'action_dim': None, 'if_discrete': None}\n",
    "        self.env_name = env_args['env_name']  # the name of environment. Be used to set 'cwd'.\n",
    "        self.state_dim = env_args['state_dim']  # vector dimension (feature number) of state\n",
    "        self.action_dim = env_args['action_dim']  # vector dimension (feature number) of action\n",
    "        self.if_discrete = env_args['if_discrete']  # discrete or continuous action space\n",
    "\n",
    "        self.agent_class = agent_class  # agent = agent_class(...)\n",
    "\n",
    "        '''Arguments for reward shaping'''\n",
    "        self.gamma = 0.99  # discount factor of future rewards\n",
    "        self.reward_scale = 1.0  # an approximate target reward usually be closed to 256\n",
    "\n",
    "        '''Arguments for training'''\n",
    "        self.gpu_id = int(0)  # `int` means the ID of single GPU, -1 means CPU\n",
    "        self.net_dims = (64, 32)  # the middle layer dimension of MLP (MultiLayer Perceptron)\n",
    "        self.learning_rate = 6e-5  # 2 ** -14 ~= 6e-5\n",
    "        self.soft_update_tau = 5e-3  # 2 ** -8 ~= 5e-3\n",
    "        self.batch_size = int(128)  # num of transitions sampled from replay buffer.\n",
    "        self.horizon_len = int(2000)  # collect horizon_len step while exploring, then update network\n",
    "        self.buffer_size = None  # ReplayBuffer size. Empty the ReplayBuffer for on-policy.\n",
    "        self.repeat_times = 8.0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "\n",
    "        '''Arguments for evaluate'''\n",
    "        self.cwd = None  # current working directory to save model. None means set automatically\n",
    "        self.break_step = +np.inf  # break training if 'total_step > break_step'\n",
    "        self.eval_times = int(32)  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = int(2e4)  # evaluate the agent per training steps\n",
    "\n",
    "    def init_before_training(self):\n",
    "        if self.cwd is None:  # set cwd (current working directory) for saving model\n",
    "            self.cwd = f'./{self.env_name}_{self.agent_class.__name__[5:]}'\n",
    "        os.makedirs(self.cwd, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_gym_env_args(env, if_print: bool) -> dict:\n",
    "    if {'unwrapped', 'observation_space', 'action_space', 'spec'}.issubset(dir(env)):  # isinstance(env, gym.Env):\n",
    "        env_name = env.unwrapped.spec.id\n",
    "        state_shape = env.observation_space.shape\n",
    "        state_dim = state_shape[0] if len(state_shape) == 1 else state_shape  # sometimes state_dim is a list\n",
    "\n",
    "        if_discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "        if if_discrete:  # make sure it is discrete action space\n",
    "            action_dim = env.action_space.n\n",
    "        elif isinstance(env.action_space, gym.spaces.Box):  # make sure it is continuous action space\n",
    "            action_dim = env.action_space.shape[0]\n",
    "\n",
    "    env_args = {'env_name': env_name, 'state_dim': state_dim, 'action_dim': action_dim, 'if_discrete': if_discrete}\n",
    "    print(f\"env_args = {repr(env_args)}\") if if_print else None\n",
    "    return env_args\n",
    "\n",
    "\n",
    "def kwargs_filter(function, kwargs: dict) -> dict:\n",
    "    import inspect\n",
    "    sign = inspect.signature(function).parameters.values()\n",
    "    sign = {val.name for val in sign}\n",
    "    common_args = sign.intersection(kwargs.keys())\n",
    "    return {key: kwargs[key] for key in common_args}  # filtered kwargs\n",
    "\n",
    "\n",
    "def build_env(env_class=None, env_args=None):\n",
    "    if env_class.__module__ == 'gym.envs.registration':  # special rule\n",
    "        env = env_class(id=env_args['env_name'])\n",
    "    else:\n",
    "        env = env_class(**kwargs_filter(env_class.__init__, env_args.copy()))\n",
    "    for attr_str in ('env_name', 'state_dim', 'action_dim', 'if_discrete'):\n",
    "        setattr(env, attr_str, env_args[attr_str])\n",
    "    return env\n",
    "\n",
    "\n",
    "class AgentBase:\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.gamma = args.gamma\n",
    "        self.batch_size = args.batch_size\n",
    "        self.repeat_times = args.repeat_times\n",
    "        self.reward_scale = args.reward_scale\n",
    "        self.soft_update_tau = args.soft_update_tau\n",
    "\n",
    "        self.states = None  # assert self.states == (1, state_dim)\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        act_class = getattr(self, \"act_class\", None)\n",
    "        cri_class = getattr(self, \"cri_class\", None)\n",
    "        self.act = self.act_target = act_class(net_dims, state_dim, action_dim).to(self.device)\n",
    "        self.cri = self.cri_target = cri_class(net_dims, state_dim, action_dim).to(self.device) \\\n",
    "            if cri_class else self.act\n",
    "\n",
    "        self.act_optimizer = torch.optim.Adam(self.act.parameters(), args.learning_rate)\n",
    "        self.cri_optimizer = torch.optim.Adam(self.cri.parameters(), args.learning_rate) \\\n",
    "            if cri_class else self.act_optimizer\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer_update(optimizer, objective: Tensor):\n",
    "        optimizer.zero_grad()\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(target_net: torch.nn.Module, current_net: torch.nn.Module, tau: float):\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
    "\n",
    "\n",
    "class AgentPPO(AgentBase):\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.if_off_policy = False\n",
    "        self.act_class = getattr(self, \"act_class\", ActorPPO)\n",
    "        self.cri_class = getattr(self, \"cri_class\", CriticPPO)\n",
    "        AgentBase.__init__(self, net_dims, state_dim, action_dim, gpu_id, args)\n",
    "\n",
    "        self.ratio_clip = getattr(args, \"ratio_clip\", 0.25)  # `ratio.clamp(1 - clip, 1 + clip)`\n",
    "        self.lambda_gae_adv = getattr(args, \"lambda_gae_adv\", 0.95)  # could be 0.80~0.99\n",
    "        self.lambda_entropy = getattr(args, \"lambda_entropy\", 0.01)  # could be 0.00~0.10\n",
    "        self.lambda_entropy = torch.tensor(self.lambda_entropy, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def explore_env(self, env, horizon_len: int) -> [Tensor]:\n",
    "        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.action_dim), dtype=torch.float32).to(self.device)\n",
    "        logprobs = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros(horizon_len, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        ary_state = self.states[0]\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        convert = self.act.convert_action_for_env\n",
    "        for i in range(horizon_len):\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n",
    "            action, logprob = [t.squeeze(0) for t in get_action(state.unsqueeze(0))[:2]]\n",
    "\n",
    "            ary_action = convert(action).detach().cpu().numpy()\n",
    "            ary_state, reward, done, _, _ = env.step(ary_action)\n",
    "            if done:\n",
    "                ary_state, _ = env.reset()\n",
    "\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            logprobs[i] = logprob\n",
    "            rewards[i] = reward\n",
    "            dones[i] = done\n",
    "\n",
    "        self.states[0] = ary_state\n",
    "        rewards = (rewards * self.reward_scale).unsqueeze(1)\n",
    "        undones = (1 - dones.type(torch.float32)).unsqueeze(1)\n",
    "        return states, actions, logprobs, rewards, undones\n",
    "\n",
    "    def update_net(self, buffer) -> [float]:\n",
    "        with torch.no_grad():\n",
    "            states, actions, logprobs, rewards, undones = buffer\n",
    "            buffer_size = states.shape[0]\n",
    "\n",
    "            '''get advantages reward_sums'''\n",
    "            bs = 2 ** 10  # set a smaller 'batch_size' when out of GPU memory.\n",
    "            values = [self.cri(states[i:i + bs]) for i in range(0, buffer_size, bs)]\n",
    "            values = torch.cat(values, dim=0).squeeze(1)  # values.shape == (buffer_size, )\n",
    "\n",
    "            advantages = self.get_advantages(rewards, undones, values)  # advantages.shape == (buffer_size, )\n",
    "            reward_sums = advantages + values  # reward_sums.shape == (buffer_size, )\n",
    "            del rewards, undones, values\n",
    "\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std(dim=0) + 1e-5)\n",
    "        assert logprobs.shape == advantages.shape == reward_sums.shape == (buffer_size,)\n",
    "        '''update network'''\n",
    "        obj_critics = 0.0\n",
    "        obj_actors = 0.0\n",
    "\n",
    "        update_times = int(buffer_size * self.repeat_times / self.batch_size)\n",
    "        assert update_times >= 1\n",
    "        for _ in range(update_times):\n",
    "            indices = torch.randint(buffer_size, size=(self.batch_size,), requires_grad=False)\n",
    "            state = states[indices]\n",
    "            action = actions[indices]\n",
    "            logprob = logprobs[indices]\n",
    "            advantage = advantages[indices]\n",
    "            reward_sum = reward_sums[indices]\n",
    "\n",
    "            value = self.cri(state).squeeze(1)  # critic network predicts the reward_sum (Q value) of state\n",
    "            obj_critic = self.criterion(value, reward_sum)\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "\n",
    "            new_logprob, obj_entropy = self.act.get_logprob_entropy(state, action)\n",
    "            ratio = (new_logprob - logprob.detach()).exp()\n",
    "            surrogate1 = advantage * ratio\n",
    "            surrogate2 = advantage * ratio.clamp(1 - self.ratio_clip, 1 + self.ratio_clip)\n",
    "            obj_surrogate = torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            obj_actor = obj_surrogate + obj_entropy.mean() * self.lambda_entropy\n",
    "            self.optimizer_update(self.act_optimizer, -obj_actor)\n",
    "\n",
    "            obj_critics += obj_critic.item()\n",
    "            obj_actors += obj_actor.item()\n",
    "        a_std_log = getattr(self.act, 'a_std_log', torch.zeros(1)).mean()\n",
    "        return obj_critics / update_times, obj_actors / update_times, a_std_log.item()\n",
    "\n",
    "    def get_advantages(self, rewards: Tensor, undones: Tensor, values: Tensor) -> Tensor:\n",
    "        advantages = torch.empty_like(values)  # advantage value\n",
    "\n",
    "        masks = undones * self.gamma\n",
    "        horizon_len = rewards.shape[0]\n",
    "\n",
    "        next_state = torch.tensor(self.states, dtype=torch.float32).to(self.device)\n",
    "        next_value = self.cri(next_state).detach()[0, 0]\n",
    "\n",
    "        advantage = 0  # last_gae_lambda\n",
    "        for t in range(horizon_len - 1, -1, -1):\n",
    "            delta = rewards[t] + masks[t] * next_value - values[t]\n",
    "            advantages[t] = advantage = delta + masks[t] * self.lambda_gae_adv * advantage\n",
    "            next_value = values[t]\n",
    "        return advantages\n",
    "\n",
    "class PendulumEnv(gym.Wrapper):  # a demo of custom gym env\n",
    "    def __init__(self):\n",
    "        gym.logger.set_level(40)  # Block warning\n",
    "        gym_env_name = \"Pendulum-v0\" if gym.__version__ < '0.18.0' else \"Pendulum-v1\"\n",
    "        super().__init__(env=gym.make(gym_env_name))\n",
    "\n",
    "        '''the necessary env information when you design a custom env'''\n",
    "        self.env_name = gym_env_name  # the name of this env.\n",
    "        self.state_dim = self.observation_space.shape[0]  # feature number of state\n",
    "        self.action_dim = self.action_space.shape[0]  # feature number of action\n",
    "        self.if_discrete = False  # discrete action or continuous action\n",
    "\n",
    "    def reset(self) -> np.ndarray:  # reset the agent in env\n",
    "        resetted_env, _ = self.env.reset()\n",
    "        return resetted_env\n",
    "\n",
    "    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):  # agent interacts in env\n",
    "        # We suggest that adjust action space to (-1, +1) when designing a custom env.\n",
    "        state, reward, done, info_dict, _ = self.env.step(action * 2)\n",
    "        return state.reshape(self.state_dim), float(reward), done, info_dict\n",
    "\n",
    "    \n",
    "def train_agent(args: Config):\n",
    "    args.init_before_training()\n",
    "\n",
    "    env = build_env(args.env_class, args.env_args)\n",
    "    agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n",
    "\n",
    "    new_env, _ = env.reset()\n",
    "    agent.states = new_env[np.newaxis, :]\n",
    "\n",
    "    evaluator = Evaluator(eval_env=build_env(args.env_class, args.env_args),\n",
    "                          eval_per_step=args.eval_per_step,\n",
    "                          eval_times=args.eval_times,\n",
    "                          cwd=args.cwd)\n",
    "    torch.set_grad_enabled(False)\n",
    "    while True: # start training\n",
    "        buffer_items = agent.explore_env(env, args.horizon_len)\n",
    "        torch.set_grad_enabled(True)\n",
    "        logging_tuple = agent.update_net(buffer_items)\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        evaluator.evaluate_and_save(agent.act, args.horizon_len, logging_tuple)\n",
    "        if (evaluator.total_step > args.break_step) or os.path.exists(f\"{args.cwd}/stop\"):\n",
    "            torch.save(agent.act.state_dict(), args.cwd + '/actor.pth')\n",
    "            break  # stop training when reach `break_step` or `mkdir cwd/stop`\n",
    "\n",
    "\n",
    "def render_agent(env_class, env_args: dict, net_dims: [int], agent_class, actor_path: str, render_times: int = 8):\n",
    "    env = build_env(env_class, env_args)\n",
    "\n",
    "    state_dim = env_args['state_dim']\n",
    "    action_dim = env_args['action_dim']\n",
    "    agent = agent_class(net_dims, state_dim, action_dim, gpu_id=-1)\n",
    "    actor = agent.act\n",
    "\n",
    "    print(f\"| render and load actor from: {actor_path}\")\n",
    "    actor.load_state_dict(torch.load(actor_path, map_location=lambda storage, loc: storage))\n",
    "    for i in range(render_times):\n",
    "        cumulative_reward, episode_step = get_rewards_and_steps(env, actor, if_render=True)\n",
    "        print(f\"|{i:4}  cumulative_reward {cumulative_reward:9.3f}  episode_step {episode_step:5.0f}\")\n",
    "\n",
    "        \n",
    "class Evaluator:\n",
    "    def __init__(self, eval_env, eval_per_step: int = 1e4, eval_times: int = 8, cwd: str = '.'):\n",
    "        self.cwd = cwd\n",
    "        self.env_eval = eval_env\n",
    "        self.eval_step = 0\n",
    "        self.total_step = 0\n",
    "        self.start_time = time.time()\n",
    "        self.eval_times = eval_times  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = eval_per_step  # evaluate the agent per training steps\n",
    "\n",
    "        self.recorder = []\n",
    "        print(f\"\\n| `step`: Number of samples, or total training steps, or running times of `env.step()`.\"\n",
    "              f\"\\n| `time`: Time spent from the start of training to this moment.\"\n",
    "              f\"\\n| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              f\"\\n| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              f\"\\n| `avgS`: Average of steps in an episode.\"\n",
    "              f\"\\n| `objC`: Objective of Critic network. Or call it loss function of critic network.\"\n",
    "              f\"\\n| `objA`: Objective of Actor network. It is the average Q value of the critic network.\"\n",
    "              f\"\\n| {'step':>8}  {'time':>8}  | {'avgR':>8}  {'stdR':>6}  {'avgS':>6}  | {'objC':>8}  {'objA':>8}\")\n",
    "            \n",
    "    def evaluate_and_save(self, actor, horizon_len: int, logging_tuple: tuple):\n",
    "        self.total_step += horizon_len\n",
    "        if self.eval_step + self.eval_per_step > self.total_step:\n",
    "            return\n",
    "        self.eval_step = self.total_step\n",
    "\n",
    "        rewards_steps_ary = [get_rewards_and_steps(self.env_eval, actor) for _ in range(self.eval_times)]\n",
    "        rewards_steps_ary = np.array(rewards_steps_ary, dtype=np.float32)\n",
    "        avg_r = rewards_steps_ary[:, 0].mean()  # average of cumulative rewards\n",
    "        std_r = rewards_steps_ary[:, 0].std()  # std of cumulative rewards\n",
    "        avg_s = rewards_steps_ary[:, 1].mean()  # average of steps in an episode\n",
    "\n",
    "        used_time = time.time() - self.start_time\n",
    "        self.recorder.append((self.total_step, used_time, avg_r))\n",
    "        \n",
    "        print(f\"| {self.total_step:8.2e}  {used_time:8.0f}  \"\n",
    "              f\"| {avg_r:8.2f}  {std_r:6.2f}  {avg_s:6.0f}  \"\n",
    "              f\"| {logging_tuple[0]:8.2f}  {logging_tuple[1]:8.2f}\")\n",
    "\n",
    "\n",
    "def get_rewards_and_steps(env, actor, if_render: bool = False) -> (float, int):  # cumulative_rewards and episode_steps\n",
    "    device = next(actor.parameters()).device  # net.parameters() is a Python generator.\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    episode_steps = 0\n",
    "    cumulative_returns = 0.0  # sum of rewards in an episode\n",
    "    for episode_steps in range(12345):\n",
    "        tensor_state = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        tensor_action = actor(tensor_state)\n",
    "        action = tensor_action.detach().cpu().numpy()[0]  # not need detach(), because using torch.no_grad() outside\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        cumulative_returns += reward\n",
    "\n",
    "        if if_render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "    return cumulative_returns, episode_steps + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f548131-20ab-47b5-8a94-33b782432ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "# from elegantrl.agents import AgentA2C\n",
    "\n",
    "# from actor_ppo import (\n",
    "#     AgentPPO,\n",
    "#     Config,\n",
    "#     train_agent,\n",
    "# )\n",
    "\n",
    "MODELS = {\"ppo\": AgentPPO}\n",
    "OFF_POLICY_MODELS = [\"ddpg\", \"td3\", \"sac\"]\n",
    "ON_POLICY_MODELS = [\"ppo\"]\n",
    "# MODEL_KWARGS = {x: config.__dict__[f\"{x.upper()}_PARAMS\"] for x in MODELS.keys()}\n",
    "#\n",
    "# NOISE = {\n",
    "#     \"normal\": NormalActionNoise,\n",
    "#     \"ornstein_uhlenbeck\": OrnsteinUhlenbeckActionNoise,\n",
    "# }\n",
    "\n",
    "\n",
    "class DRLAgent:\n",
    "    \"\"\"Implementations of DRL algorithms\n",
    "    Attributes\n",
    "    ----------\n",
    "        env: gym environment class\n",
    "            user-defined class\n",
    "    Methods\n",
    "    -------\n",
    "        get_model()\n",
    "            setup DRL algorithms\n",
    "        train_model()\n",
    "            train DRL algorithms in a train dataset\n",
    "            and output the trained model\n",
    "        DRL_prediction()\n",
    "            make a prediction in a test dataset and get results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, price_array, tech_array, turbulence_array):\n",
    "        self.env = env\n",
    "        self.price_array = price_array\n",
    "        self.tech_array = tech_array\n",
    "        self.turbulence_array = turbulence_array\n",
    "\n",
    "    def get_model(self, model_name, model_kwargs):\n",
    "        env_config = {\n",
    "            \"price_array\": self.price_array,\n",
    "            \"tech_array\": self.tech_array,\n",
    "            \"turbulence_array\": self.turbulence_array,\n",
    "            \"if_train\": True,\n",
    "        }\n",
    "        environment = self.env(config=env_config)\n",
    "        env_args = {'config': env_config,\n",
    "              'env_name': environment.env_name,\n",
    "              'state_dim': environment.state_dim,\n",
    "              'action_dim': environment.action_dim,\n",
    "              'if_discrete': False}\n",
    "        agent = MODELS[model_name]\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "        model = Config(agent_class=agent, env_class=self.env, env_args=env_args)\n",
    "        model.if_off_policy = model_name in OFF_POLICY_MODELS\n",
    "        if model_kwargs is not None:\n",
    "            try:\n",
    "                model.learning_rate = model_kwargs[\"learning_rate\"]\n",
    "                model.batch_size = model_kwargs[\"batch_size\"]\n",
    "                model.gamma = model_kwargs[\"gamma\"]\n",
    "                model.seed = model_kwargs[\"seed\"]\n",
    "                model.net_dims = model_kwargs[\"net_dimension\"]\n",
    "                model.target_step = model_kwargs[\"target_step\"]\n",
    "                model.eval_gap = model_kwargs[\"eval_gap\"]\n",
    "                model.eval_times = model_kwargs[\"eval_times\"]\n",
    "            except BaseException:\n",
    "                raise ValueError(\n",
    "                    \"Fail to read arguments, please check 'model_kwargs' input.\"\n",
    "                )\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, cwd, total_timesteps=5000):\n",
    "        model.cwd = cwd\n",
    "        model.break_step = total_timesteps\n",
    "        train_agent(model)\n",
    "\n",
    "    @staticmethod\n",
    "    def DRL_prediction(model_name, cwd, net_dimension, environment):\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "        agent_class = MODELS[model_name]\n",
    "        environment.env_num = 1\n",
    "        agent = agent_class(net_dimension, environment.state_dim, environment.action_dim)\n",
    "        actor = agent.act\n",
    "        # load agent\n",
    "        try:\n",
    "            cwd = cwd + '/actor.pth'\n",
    "            print(f\"| load actor from: {cwd}\")\n",
    "            actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n",
    "            act = actor\n",
    "            device = agent.device\n",
    "        except BaseException:\n",
    "            raise ValueError(\"Fail to load agent!\")\n",
    "\n",
    "        # test on the testing env\n",
    "        _torch = torch\n",
    "        state, _ = environment.reset()\n",
    "        episode_returns = []  # the cumulative_return / initial_account\n",
    "        episode_total_assets = [environment.initial_total_asset]\n",
    "        with _torch.no_grad():\n",
    "            for i in range(environment.max_step):\n",
    "                s_tensor = _torch.as_tensor((state,), device=device)\n",
    "                a_tensor = act(s_tensor)  # action_tanh = act.forward()\n",
    "                action = (\n",
    "                    a_tensor.detach().cpu().numpy()[0]\n",
    "                )  # not need detach(), because with torch.no_grad() outside\n",
    "                state, reward, done, _, _ = environment.step(action)\n",
    "\n",
    "                total_asset = (\n",
    "                    environment.amount\n",
    "                    + (\n",
    "                        environment.price_ary[environment.day] * environment.stocks\n",
    "                    ).sum()\n",
    "                )\n",
    "                episode_total_assets.append(total_asset)\n",
    "                episode_return = total_asset / environment.initial_total_asset\n",
    "                episode_returns.append(episode_return)\n",
    "                if done:\n",
    "                    break\n",
    "        print(\"Test Finished!\")\n",
    "        # return episode total_assets on testing data\n",
    "        print(\"episode_return\", episode_return)\n",
    "        return episode_total_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5126714b-d61e-438d-aea9-719f2237560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def df_to_array(df, tech_indicator_list):\n",
    "    df = df.copy()\n",
    "    unique_ticker = df.tic.unique()\n",
    "    if_first_time = True\n",
    "    for tic in unique_ticker:\n",
    "        if if_first_time:\n",
    "            price_array = df[df.tic == tic][[\"close\"]].values\n",
    "            tech_array = df[df.tic == tic][tech_indicator_list].values\n",
    "            turbulence_array = df[df.tic == tic][\"turbulence\"].values\n",
    "            if_first_time = False\n",
    "        else:\n",
    "            price_array = np.hstack(\n",
    "                [price_array, df[df.tic == tic][[\"close\"]].values]\n",
    "            )\n",
    "            tech_array = np.hstack(\n",
    "                [tech_array, df[df.tic == tic][tech_indicator_list].values]\n",
    "            )\n",
    "    tech_nan_positions = np.isnan(tech_array)\n",
    "    tech_array[tech_nan_positions] = 0\n",
    "    tech_inf_positions = np.isinf(tech_array)\n",
    "    tech_array[tech_inf_positions] = 0\n",
    "    return price_array, tech_array, turbulence_array, unique_ticker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
